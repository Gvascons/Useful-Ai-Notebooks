{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio 01 - RNN, LSTM e GRU\n",
    "\n",
    "Aluno: **João Gabriel de Araújo Vasconcelos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode ser feito individualmente ou em dupla. Entender a implementação \"from  scratch\" da RNN, LSTM e GRU (ver links do kaggle abaixo e/ou o livro online: https://d2l.ai/ ).\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\n",
    "\n",
    "https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\n",
    "\n",
    "https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\n",
    "\n",
    "\n",
    "Executar o códigos nas respectivas bases de dados. Executar também em bases de dados diferentes das mostradas nos exemplos (explicar as bases no próprio notebook).\n",
    "\n",
    "A entrega consistirá nos notebooks com comentários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of clarity (maybe?), although the requirements are written in Portuguese, the rest of the notebook will be in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a DataGenerator class that handles loading and preprocessing text data for training the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for generating input and output examples for a character-level language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Initializes a DataGenerator object.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file containing the training data.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        \n",
    "        # Read in data from file and convert to lowercase\n",
    "        with open(path) as f:\n",
    "            data = f.read().lower()\n",
    "        \n",
    "        # Create list of unique characters in the data\n",
    "        self.chars = list(set(data))\n",
    "        \n",
    "        # Create dictionaries mapping characters to and from their index in the list of unique characters\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(self.chars)}\n",
    "        \n",
    "        # Set the size of the vocabulary (i.e. number of unique characters)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Read in examples from file and convert to lowercase, removing leading/trailing white space\n",
    "        with open(path) as f:\n",
    "            examples = f.readlines()\n",
    "        self.examples = [x.lower().strip() for x in examples]\n",
    " \n",
    "    def generate_example(self, idx):\n",
    "        \"\"\"\n",
    "        Generates an input/output example for the language model based on the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the example to generate.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the input and output arrays for the example.\n",
    "        \"\"\"\n",
    "        example_chars = self.examples[idx]\n",
    "        \n",
    "        # Convert the characters in the example to their corresponding indices in the list of unique characters\n",
    "        example_char_idx = [self.char_to_idx[char] for char in example_chars]\n",
    "        \n",
    "        # Add newline character as the first character in the input array, and as the last character in the output array\n",
    "        X = [self.char_to_idx['\\n']] + example_char_idx\n",
    "        Y = example_char_idx + [self.char_to_idx['\\n']]\n",
    "        \n",
    "        return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells define and implement the Recurrent Neural Network (RNN) class, including initialization, forward and backward propagation, training, and prediction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (RNN).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the RNN.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the RNN.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the RNN.\n",
    "    learning_rate : float\n",
    "        The learning rate used during training.\n",
    "    is_initialized : bool\n",
    "        Indicates whether the AdamW parameters has been initialized.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, learning_rate)\n",
    "        Initializes an instance of the RNN class.\n",
    "    \n",
    "    forward(self, X, a_prev)\n",
    "     Computes the forward pass of the RNN.\n",
    "     \n",
    "    softmax(self, x)\n",
    "       Computes the softmax activation function for a given input array. \n",
    "       \n",
    "    backward(self,x, a, y_preds, targets)    \n",
    "        Implements the backward pass of the RNN.\n",
    "        \n",
    "   loss(self, y_preds, targets)\n",
    "     Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets. \n",
    "     \n",
    "    adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4)\n",
    "       Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "       \n",
    "    train(self, generated_names=5)\n",
    "       Trains the RNN on a dataset using backpropagation through time (BPTT).   \n",
    "       \n",
    "   predict(self, start)\n",
    "        Generates a sequence of characters using the trained self, starting from the given start sequence.\n",
    "        The generated sequence may contain a maximum of 50 characters or a newline character.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, data_generator, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the RNN class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the RNN.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the RNN.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the RNN.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.data_generator = data_generator\n",
    "        self.vocab_size = self.data_generator.vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "\n",
    "        # model parameters\n",
    "        self.Wax = np.random.uniform(-np.sqrt(1. / self.vocab_size), np.sqrt(1. / self.vocab_size), (hidden_size, self.vocab_size))\n",
    "        self.Waa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (hidden_size, hidden_size))\n",
    "        self.Wya = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (self.vocab_size, hidden_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))  \n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.dWax, self.dWaa, self.dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        self.dba, self.dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        \n",
    "        # parameter update with AdamW\n",
    "        self.mWax = np.zeros_like(self.Wax)\n",
    "        self.vWax = np.zeros_like(self.Wax)\n",
    "        self.mWaa = np.zeros_like(self.Waa)\n",
    "        self.vWaa = np.zeros_like(self.Waa)\n",
    "        self.mWya = np.zeros_like(self.Wya)\n",
    "        self.vWya = np.zeros_like(self.Wya)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, a_prev):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the RNN.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Input data of shape (seq_length, vocab_size)\n",
    "        a_prev (ndarray): Activation of the previous time step of shape (hidden_size, 1)\n",
    "\n",
    "        Returns:\n",
    "        x (dict): Dictionary of input data of shape (seq_length, vocab_size, 1), with keys from 0 to seq_length-1\n",
    "        a (dict): Dictionary of hidden activations for each time step, with keys from 0 to seq_length-1\n",
    "        y_pred (dict): Dictionary of output probabilities for each time step, with keys from 0 to seq_length-1\n",
    "        \"\"\"\n",
    "        # Initialize dictionaries to store activations and output probabilities.\n",
    "        x, a, y_pred = {}, {}, {}\n",
    "\n",
    "        # Store the input data in the class variable for later use in the backward pass.\n",
    "        self.X = X\n",
    "\n",
    "        # Set the initial activation to the previous activation.\n",
    "        a[-1] = np.copy(a_prev)\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(len(self.X)): \n",
    "            # get the input at the current time step\n",
    "            x[t] = np.zeros((self.vocab_size,1)) \n",
    "            if (self.X[t] != None):\n",
    "                x[t][self.X[t]] = 1\n",
    "            # compute the hidden activation at the current time step\n",
    "            a[t] = np.tanh(np.dot(self.Wax, x[t]) + np.dot(self.Waa, a[t - 1]) + self.ba)\n",
    "            # compute the output probabilities at the current time step\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wya, a[t]) + self.by)\n",
    "            # add an extra dimension to X to make it compatible with the shape of the input to the backward pass\n",
    "         # return the input, hidden activations, and output probabilities at each time step\n",
    "        return x, a, y_pred \n",
    "    \n",
    "    def backward(self,x, a, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Implement the backward pass of the RNN.\n",
    "\n",
    "        Args:\n",
    "        x -- (dict) of input characters (as one-hot encoding vectors) for each time-step, shape (vocab_size, sequence_length)\n",
    "        a -- (dict) of hidden state vectors for each time-step, shape (hidden_size, sequence_length)\n",
    "        y_preds -- (dict) of output probability vectors (after softmax) for each time-step, shape (vocab_size, sequence_length)\n",
    "        targets -- (list) of integer target characters (indices of characters in the vocabulary) for each time-step, shape (1, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize derivative of hidden state for the last time-step\n",
    "        da_next = np.zeros_like(a[0])\n",
    "\n",
    "        # Loop through the input sequence backwards\n",
    "        for t in reversed(range(len(self.X))):\n",
    "            # Calculate derivative of output probability vector\n",
    "            dy_preds = np.copy(y_preds[t])\n",
    "            dy_preds[targets[t]] -= 1\n",
    "\n",
    "            # Calculate derivative of hidden state\n",
    "            da = np.dot(self.Waa.T, da_next) + np.dot(self.Wya.T, dy_preds)\n",
    "            dtanh = (1 - np.power(a[t], 2))\n",
    "            da_unactivated = dtanh * da\n",
    "\n",
    "            # Calculate gradients\n",
    "            self.dba += da_unactivated\n",
    "            self.dWax += np.dot(da_unactivated, x[t].T)\n",
    "            self.dWaa += np.dot(da_unactivated, a[t - 1].T)\n",
    "\n",
    "            # Update derivative of hidden state for the next iteration\n",
    "            da_next = da_unactivated\n",
    "\n",
    "            # Calculate gradient for output weight matrix\n",
    "            self.dWya += np.dot(dy_preds, a[t].T)\n",
    "\n",
    "            # clip gradients to avoid exploding gradients\n",
    "            for grad in [self.dWax, self.dWaa, self.dWya, self.dba, self.dby]:\n",
    "                np.clip(grad, -1, 1, out=grad)\n",
    " \n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # calculate cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(len(self.X)))\n",
    "    \n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        # AdamW update for Wax\n",
    "        self.mWax = beta1 * self.mWax + (1 - beta1) * self.dWax\n",
    "        self.vWax = beta2 * self.vWax + (1 - beta2) * np.square(self.dWax)\n",
    "        m_hat = self.mWax / (1 - beta1)\n",
    "        v_hat = self.vWax / (1 - beta2)\n",
    "        self.Wax -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wax)\n",
    "\n",
    "        # AdamW update for Waa\n",
    "        self.mWaa = beta1 * self.mWaa + (1 - beta1) * self.dWaa\n",
    "        self.vWaa = beta2 * self.vWaa + (1 - beta2) * np.square(self.dWaa)\n",
    "        m_hat = self.mWaa / (1 - beta1)\n",
    "        v_hat = self.vWaa / (1 - beta2)\n",
    "        self.Waa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Waa)\n",
    "\n",
    "        # AdamW update for Wya\n",
    "        self.mWya = beta1 * self.mWya + (1 - beta1) * self.dWya\n",
    "        self.vWya = beta2 * self.vWya + (1 - beta2) * np.square(self.dWya)\n",
    "        m_hat = self.mWya / (1 - beta1)\n",
    "        v_hat = self.vWya / (1 - beta2)\n",
    "        self.Wya -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wya)\n",
    "\n",
    "        # AdamW update for ba\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Sample a sequence of characters from the RNN.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "        \"\"\"\n",
    "        # initialize input and hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # create an empty list to store the generated character indices\n",
    "        indices = []\n",
    "\n",
    "        # idx is a flag to detect a newline character, initialize it to -1\n",
    "        idx = -1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        counter = 0\n",
    "        max_chars = 50 # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n'] # the newline character\n",
    "\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # compute the hidden state\n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(list(range(self.vocab_size)), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # store the sampled character index in the list\n",
    "            indices.append(idx)\n",
    "\n",
    "            # update the previous hidden state\n",
    "            a_prev = a\n",
    "\n",
    "            # increment the counter\n",
    "            counter += 1\n",
    "\n",
    "        # return the list of sampled character indices\n",
    "        return indices\n",
    "\n",
    "        \n",
    "    def train(self, generated_names=5):\n",
    "        \"\"\"\n",
    "        Train the RNN on a dataset using backpropagation through time (BPTT).\n",
    "\n",
    "        Args:\n",
    "        - generated_names: an integer indicating how many example names to generate during training.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "        iter_num = 0\n",
    "        threshold = 5 # stopping criterion for training\n",
    "        smooth_loss = -np.log(1.0 / self.data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "\n",
    "        while (smooth_loss > threshold):\n",
    "            a_prev = np.zeros((self.hidden_size, 1))\n",
    "            idx = iter_num % self.vocab_size\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = self.data_generator.generate_example(idx)\n",
    "\n",
    "            # forward pass\n",
    "            x, a, y_pred  = self.forward(inputs, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(x, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[len(self.X) - 1]\n",
    "            # print progress every 500 iterations\n",
    "            if iter_num % 500 == 0:\n",
    "                print(\"\\n\\niter :%d, loss:%f\\n\" % (iter_num, smooth_loss))\n",
    "                for i in range(generated_names):\n",
    "                    sample_idx = self.sample()\n",
    "                    txt = ''.join(self.data_generator.idx_to_char[idx] for idx in sample_idx)\n",
    "                    txt = txt.title()  # capitalize first character \n",
    "                    print ('%s' % (txt, ), end='')\n",
    "            iter_num += 1\n",
    "    \n",
    "    def predict(self, start):\n",
    "        \"\"\"\n",
    "        Generate a sequence of characters using the trained self, starting from the given start sequence.\n",
    "        The generated sequence may contain a maximum of 50 characters or a newline character.\n",
    "\n",
    "        Args:\n",
    "        - start: a string containing the start sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize input vector and previous hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Convert start sequence to indices\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = self.data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # Generate sequence\n",
    "        max_chars = 50  # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n']  # the newline character\n",
    "        counter = 0\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # Compute next hidden state and predicted character\n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "            y_pred = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "\n",
    "            # Update input vector, previous hidden state, and indices\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            a_prev = a\n",
    "            idxes.append(idx)\n",
    "            counter += 1\n",
    "\n",
    "        # Convert indices to characters and concatenate into a string\n",
    "        txt = ''.join(self.data_generator.idx_to_char[i] for i in idxes)\n",
    "\n",
    "        # Remove newline character if it exists at the end of the generated sequence\n",
    "        if txt[-1] == '\\n':\n",
    "            txt = txt[:-1]\n",
    "\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iter :0, loss:82.360052\n",
      "\n",
      "Dwbwfjsxjioghrq\n",
      "Jpmjuabgehfnscuyirhifaag\n",
      "Qbdfhxejog\n",
      "RuosetzyexdecketrcffyrdjjayflzkmbggweexmjasyztrrhsYrotpubolwhhrespnzilrataczmxlyxvczd\n",
      "\n",
      "\n",
      "iter :500, loss:59.155745\n",
      "\n",
      "Freedllhottenhyoshtrusanahhlohrhmlusiurus\n",
      "Hemoloolavosatrus\n",
      "Aueinmohysneroshisabmysuheayooshrris\n",
      "\n",
      "Mednonyeaarhoeioihinaalhosacros\n",
      "\n",
      "\n",
      "iter :1000, loss:42.003127\n",
      "\n",
      "Eaeyonhxaoras\n",
      "Afrittosrurus\n",
      "Adrovaathir\n",
      "Dadaoraonooli\n",
      "Asantosaurusaurus\n",
      "\n",
      "\n",
      "iter :1500, loss:29.920424\n",
      "\n",
      "Acrosantoosaurus\n",
      "Acritleps\n",
      "Abtiolnus\n",
      "Afrollolopaurus\n",
      "Ybrasautor\n",
      "\n",
      "\n",
      "iter :2000, loss:21.498926\n",
      "\n",
      "Saurusburus\n",
      "Adamonyiabrotholus\n",
      "Achdosaurus\n",
      "Acrdanyx\n",
      "Amdistuvus\n",
      "\n",
      "\n",
      "iter :2500, loss:15.773872\n",
      "\n",
      "Abromenator\n",
      "Aatausaurus\n",
      "Acrimtor\n",
      "Acrocanalosaurus\n",
      "Abrimdalrolimyn\n",
      "\n",
      "\n",
      "iter :3000, loss:11.930649\n",
      "\n",
      "Foonyx\n",
      "Arrosaurus\n",
      "Aurasterobus\n",
      "Afrosaurus\n",
      "Abpaesaetis\n",
      "\n",
      "\n",
      "iter :3500, loss:9.378645\n",
      "\n",
      "Actoosaurus\n",
      "Aerosturus\n",
      "Ameoonopausaurus\n",
      "Abyyornithamus\n",
      "Acristurus\n",
      "\n",
      "\n",
      "iter :4000, loss:7.736533\n",
      "\n",
      "Afrovenator\n",
      "Afrovenator\n",
      "Afrovhnator\n",
      "Acristavus\n",
      "Afrovenator\n",
      "\n",
      "\n",
      "iter :4500, loss:6.613502\n",
      "\n",
      "Acantholus\n",
      "Achecaupaptor\n",
      "Auristavus\n",
      "Abrisausus\n",
      "Achillobator\n",
      "\n",
      "\n",
      "iter :5000, loss:5.893339\n",
      "\n",
      "Aldoptosaurus\n",
      "Adamantisaurus\n",
      "Acrosteon\n",
      "Aorostavus\n",
      "Abelisaurus\n",
      "\n",
      "\n",
      "iter :5500, loss:5.410912\n",
      "\n",
      "Aatin\n",
      "Aariclosaurus\n",
      "Aarisaavus\n",
      "Afrovenataocantholis\n",
      "Abrictosaurus\n",
      "\n",
      "\n",
      "iter :6000, loss:5.061576\n",
      "\n",
      "Actiosaurus\n",
      "Achillosaurus\n",
      "Adelonosaurus\n",
      "Auriotosaurus\n",
      "Acristhonix\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataGenerator and RNN\n",
    "data_generator = DataGenerator('dinos.txt')\n",
    "rnn = RNN(hidden_size=200, data_generator=data_generator, sequence_length=25, learning_rate=1e-3)\n",
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meonallesaurus'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"meo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aurovepaepi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also checking the output of \"itsbeen\": We're using \"itsbeen\" (which will vary for further models) as a consistent input across all models in this notebook in order to allow a direct comparison of output quality between different model architectures. It's a short, common phrase that doesn't overly constrain the model's generation (aside from being a \"meme\" from the final dataset). It's also open-ended enough to showcase the model's ability to generate diverse continuations, serving as a quick benchmark to assess each model's basic text generation capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>itsbeen</td>\n",
       "      <td>itsbeenachenosaurus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model    Input               Output\n",
       "0   RNN  itsbeen  itsbeenachenosaurus"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create input string\n",
    "input_string = \"itsbeen\"\n",
    "\n",
    "# Get model output\n",
    "try:\n",
    "    output = rnn.predict(input_string)\n",
    "except KeyError as e:\n",
    "    output = f\"Error: {str(e)}\"\n",
    "\n",
    "# Create DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['RNN'],\n",
    "    'Input': [input_string],\n",
    "    'Output': [output]\n",
    "})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for reading and preprocessing text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, sequence_length: int):\n",
    "        \"\"\"\n",
    "        Initializes a DataReader object with the path to a text file and the desired sequence length.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file.\n",
    "            sequence_length (int): The length of the sequences that will be fed to the self.\n",
    "        \"\"\"\n",
    "        with open(path) as f:\n",
    "            # Read the contents of the file\n",
    "            self.data = f.read()\n",
    "\n",
    "        # Find all unique characters in the text\n",
    "        chars = list(set(self.data))\n",
    "\n",
    "        # Create dictionaries to map characters to indices and vice versa\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(chars)}\n",
    "\n",
    "        # Store the size of the text data and the size of the vocabulary\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "        # Initialize the pointer that will be used to generate sequences\n",
    "        self.pointer = 0\n",
    "\n",
    "        # Store the desired sequence length\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Generates a batch of input and target sequences.\n",
    "\n",
    "        Returns:\n",
    "            inputs_one_hot (np.ndarray): A numpy array with shape `(batch_size, vocab_size)` where each row is a one-hot encoded representation of a character in the input sequence.\n",
    "            targets (list): A list of integers that correspond to the indices of the characters in the target sequence, which is the same as the input sequence shifted by one position to the right.\n",
    "        \"\"\"\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.sequence_length\n",
    "\n",
    "        # Get the input sequence as a list of integers\n",
    "        inputs = [self.char_to_idx[ch] for ch in self.data[input_start:input_end]]\n",
    "\n",
    "        # One-hot encode the input sequence\n",
    "        inputs_one_hot = np.zeros((len(inputs), self.vocab_size))\n",
    "        inputs_one_hot[np.arange(len(inputs)), inputs] = 1\n",
    "\n",
    "        # Get the target sequence as a list of integers\n",
    "        targets = [self.char_to_idx[ch] for ch in self.data[input_start + 1:input_end + 1]]\n",
    "\n",
    "        # Update the pointer\n",
    "        self.pointer += self.sequence_length\n",
    "\n",
    "        # Reset the pointer if the next batch would exceed the length of the text data\n",
    "        if self.pointer + self.sequence_length + 1 >= self.data_size:\n",
    "            self.pointer = 0\n",
    "\n",
    "        return inputs_one_hot, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells define and implement the Long Short-Term Memory (LSTM) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (LSTM).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the LSTM.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the LSTM.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the LSTM.\n",
    "    self.learning_rate : float\n",
    "        The learning rate used during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n",
    "        Initializes an instance of the LSTM class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the LSTM class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the LSTM.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the LSTM.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the LSTM.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "        # hyper parameters\n",
    "        self.mby = None\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # model parameters\n",
    "        self.Wf = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.Wi = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wc = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "            \n",
    "        self.Wo = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (vocab_size, hidden_size))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "\n",
    "        # initialize parameters for adamw optimizer\n",
    "        self.mWf = np.zeros_like(self.Wf)\n",
    "        self.vWf = np.zeros_like(self.Wf)\n",
    "        self.mWi = np.zeros_like(self.Wi)\n",
    "        self.vWi = np.zeros_like(self.Wi)\n",
    "        self.mWc = np.zeros_like(self.Wc)\n",
    "        self.vWc = np.zeros_like(self.Wc)\n",
    "        self.mWo = np.zeros_like(self.Wo)\n",
    "        self.vWo = np.zeros_like(self.Wo)\n",
    "        self.mWy = np.zeros_like(self.Wy)\n",
    "        self.vWy = np.zeros_like(self.Wy)\n",
    "        self.mbf = np.zeros_like(self.bf)\n",
    "        self.vbf = np.zeros_like(self.bf)\n",
    "        self.mbi = np.zeros_like(self.bi)\n",
    "        self.vbi = np.zeros_like(self.bi)\n",
    "        self.mbc = np.zeros_like(self.bc)\n",
    "        self.vbc = np.zeros_like(self.bc)\n",
    "        self.mbo = np.zeros_like(self.bo)\n",
    "        self.vbo = np.zeros_like(self.bo)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # calculate cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
    "\n",
    "\n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the LSTM's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        # AdamW update for Wf\n",
    "        self.mWf = beta1 * self.mWf + (1 - beta1) * self.dWf\n",
    "        self.vWf = beta2 * self.vWf + (1 - beta2) * np.square(self.dWf)\n",
    "        m_hat = self.mWf / (1 - beta1)\n",
    "        v_hat = self.vWf / (1 - beta2)\n",
    "        self.Wf -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wf)\n",
    "\n",
    "        # AdamW update for bf\n",
    "        self.mbf = beta1 * self.mbf + (1 - beta1) * self.dbf\n",
    "        self.vbf = beta2 * self.vbf + (1 - beta2) * np.square(self.dbf)\n",
    "        m_hat = self.mbf / (1 - beta1)\n",
    "        v_hat = self.vbf / (1 - beta2)\n",
    "        self.bf -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bf)\n",
    "\n",
    "        # AdamW update for Wi\n",
    "        self.mWi = beta1 * self.mWi + (1 - beta1) * self.dWi\n",
    "        self.vWi = beta2 * self.vWi + (1 - beta2) * np.square(self.dWi)\n",
    "        m_hat = self.mWi / (1 - beta1)\n",
    "        v_hat = self.vWi / (1 - beta2)\n",
    "        self.Wi -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wi)\n",
    "\n",
    "        # AdamW update for bi\n",
    "        self.mbi = beta1 * self.mbi + (1 - beta1) * self.dbi\n",
    "        self.vbi = beta2 * self.vbi + (1 - beta2) * np.square(self.dbi)\n",
    "        m_hat = self.mbi / (1 - beta1)\n",
    "        v_hat = self.vbi / (1 - beta2)\n",
    "        self.bi -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bi)\n",
    "\n",
    "        # AdamW update for Wc\n",
    "        self.mWc = beta1 * self.mWc + (1 - beta1) * self.dWc\n",
    "        self.vWc = beta2 * self.vWc + (1 - beta2) * np.square(self.dWc)\n",
    "        m_hat = self.mWc / (1 - beta1)\n",
    "        v_hat = self.vWc / (1 - beta2)\n",
    "        self.Wc -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wc)\n",
    "\n",
    "        # AdamW update for bc\n",
    "        self.mbc = beta1 * self.mbc + (1 - beta1) * self.dbc\n",
    "        self.vbc = beta2 * self.vbc + (1 - beta2) * np.square(self.dbc)\n",
    "        m_hat = self.mbc / (1 - beta1)\n",
    "        v_hat = self.vbc / (1 - beta2)\n",
    "        self.bc -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bc)\n",
    "\n",
    "        # AdamW update for Wy\n",
    "        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n",
    "        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n",
    "        m_hat = self.mWy / (1 - beta1)\n",
    "        v_hat = self.vWy / (1 - beta2)\n",
    "        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "        m_hat = self.mby / (1 - beta1)\n",
    "        v_hat = self.vby / (1 - beta2)\n",
    "        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n",
    "\n",
    "\n",
    "    def forward(self, X, c_prev, a_prev):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for a simple LSTM model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "\n",
    "        Returns:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            f (dictionary): Forget gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            i (dictionary): Input gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            o (dictionary): Output gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n",
    "        \"\"\"\n",
    "        # initialize dictionaries for backpropagation \n",
    "        c, f, i, o, cc, a, y_pred = {}, {}, {}, {}, {}, {}, {}\n",
    "        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n",
    "        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n",
    "\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(X.shape[0]):\n",
    "            # concatenate the input and hidden state\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a[t - 1], xt))\n",
    "\n",
    "            # compute the forget gate\n",
    "            f[t] = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "\n",
    "            # compute the input gate\n",
    "            i[t] = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc[t] = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "\n",
    "            # compute the cell state\n",
    "            c[t] = f[t] * c[t - 1] + i[t] * cc[t]\n",
    "\n",
    "            # compute the output gate\n",
    "            o[t] = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "\n",
    "            # compute the hidden state\n",
    "            a[t] = o[t] * np.tanh(c[t])\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n",
    "\n",
    "        # return the output probability vectors, cell state, hidden state and gate vectors\n",
    "        return X, y_pred, c, f, i, o, cc, a \n",
    "\n",
    "\n",
    "    def backward(self, X, targets, y_pred, c_prev, a_prev, c, f, i, o, cc, a):\n",
    "        \"\"\"\n",
    "        Performs backward propagation through time for an LSTM network.\n",
    "\n",
    "        Args:\n",
    "        - X: input data for each time step, with shape (sequence_length, input_size)\n",
    "        - targets: target outputs for each time step, with shape (sequence_length, output_size)\n",
    "        - y_pred: predicted outputs for each time step, with shape (sequence_length, output_size)\n",
    "        - c_prev: previous cell state, with shape (hidden_size, 1)\n",
    "        - a_prev: previous hidden state, with shape (hidden_size, 1)\n",
    "        - c: cell state for each time step, with shape (sequence_length, hidden_size)\n",
    "        - f: forget gate output for each time step, with shape (sequence_length, hidden_size)\n",
    "        - i: input gate output for each time step, with shape (sequence_length, hidden_size)\n",
    "        - o: output gate output for each time step, with shape (sequence_length, hidden_size)\n",
    "        - cc: candidate cell state for each time step, with shape (sequence_length, hidden_size)\n",
    "        - a: hidden state output for each time step, with shape (sequence_length, hidden_size)\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize gradients for each parameter\n",
    "        self.dWf, self.dWi, self.dWc, self.dWo, self.dWy = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wc), np.zeros_like(self.Wo), np.zeros_like(self.Wy)\n",
    "        self.dbf, self.dbi, self.dbc, self.dbo, self.dby = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bc), np.zeros_like(self.bo), np.zeros_like(self.by)\n",
    "        dc_next = np.zeros_like(c_prev)\n",
    "        da_next = np.zeros_like(a_prev)\n",
    "\n",
    "        # iterate backwards through time steps\n",
    "        for t in reversed(range(X.shape[0])):\n",
    "            # compute the gradient of the output probability vector\n",
    "            dy = np.copy(y_pred[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # compute the gradient of the output layer weights and biases\n",
    "            self.dWy += np.dot(dy, a[t].T)\n",
    "            self.dby += dy\n",
    "\n",
    "            # compute the gradient of the hidden state\n",
    "            da = np.dot(self.Wy.T, dy) + da_next\n",
    "            dc = dc_next + (1 - np.tanh(c[t])**2) * o[t] * da\n",
    "            \n",
    "            # compute the gradient of the output gate\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a[t - 1], xt))\n",
    "            do = o[t] * (1 - o[t]) * np.tanh(c[t]) * da\n",
    "            self.dWo += np.dot(do, concat.T)\n",
    "            self.dbo += do\n",
    "\n",
    "            # compute the gradient of the candidate cell state\n",
    "            dcc = dc * i[t] * (1 - np.tanh(cc[t])**2)\n",
    "            self.dWc += np.dot(dcc, concat.T)\n",
    "            self.dbc += dcc\n",
    "\n",
    "            # compute the gradient of the input gate\n",
    "            di = i[t] * (1 - i[t]) * cc[t] * dc\n",
    "            self.dWi += np.dot(di, concat.T)\n",
    "            self.dbi += di\n",
    "\n",
    "            # compute the gradient of the forget gate\n",
    "            df = f[t] * (1 - f[t]) * c[t - 1] * dc\n",
    "            self.dWf += np.dot(df, concat.T)\n",
    "            self.dbf += df\n",
    "\n",
    "            # compute the gradient of the input to the current hidden state and cell state\n",
    "            da_next = np.dot(self.Wf[:, :self.hidden_size].T, df)\\\n",
    "            + np.dot(self.Wi[:, :self.hidden_size].T, di)\\\n",
    "            + np.dot(self.Wc[:, :self.hidden_size].T, dcc)\\\n",
    "            + np.dot(self.Wo[:, :self.hidden_size].T, do)\n",
    "            dc_next = dc * f[t]\n",
    "\n",
    "        # clip gradients to avoid exploding gradients\n",
    "        for grad in [self.dWf, self.dWi, self.dWc, self.dWo, self.dWy, self.dbf, self.dbi, self.dbc, self.dbo, self.dby]:\n",
    "            np.clip(grad, -1, 1, out=grad)\n",
    "\n",
    "\n",
    "    def train(self, data_generator):\n",
    "        \"\"\"\n",
    "        Train the LSTM on a dataset using backpropagation through time.\n",
    "\n",
    "        Args:\n",
    "            data_generator: An instance of DataGenerator containing the training data.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        iter_num = 0\n",
    "        # stopping criterion for training\n",
    "        threshold = 46\n",
    "        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "        while (smooth_loss > threshold):\n",
    "            # initialize hidden state at the beginning of each sequence\n",
    "            if data_generator.pointer == 0:\n",
    "                c_prev = np.zeros((self.hidden_size, 1))\n",
    "                a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = data_generator.next_batch()\n",
    "\n",
    "            # forward pass\n",
    "            X, y_pred, c, f, i, o, cc, a   = self.forward(inputs, c_prev, a_prev)\n",
    "        \n",
    "            # backward pass\n",
    "            self.backward( X, targets, y_pred, c_prev, a_prev, c, f, i, o, cc, a)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[self.sequence_length - 1]\n",
    "            c_prev = c[self.sequence_length - 1]\n",
    "            # print progress every 1000 iterations\n",
    "            if iter_num % 1000 == 0:\n",
    "                self.learning_rate *= 0.99\n",
    "                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n",
    "                print(''.join(data_generator.idx_to_char[idx] for idx in sample_idx))\n",
    "                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
    "            iter_num += 1\n",
    "\n",
    "            \n",
    "    def sample(self, c_prev, a_prev, seed_idx, n):\n",
    "        \"\"\"\n",
    "        Sample a sequence of integers from the model.\n",
    "\n",
    "        Args:\n",
    "            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n",
    "            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n",
    "            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n",
    "            n (int): Number of characters to generate.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize input and seed_idx\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        # convert one-hot encoding to integer index\n",
    "        seed_idx = np.argmax(seed_idx, axis=-1)\n",
    "\n",
    "        # set the seed letter as the input for the first time step\n",
    "        x[seed_idx] = 1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        idxes = []\n",
    "        c = np.copy(c_prev)\n",
    "        a = np.copy(a_prev)\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "            i = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "            f = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "            cc = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "            c = f * c + i * cc\n",
    "            o = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "            a = o * np.tanh(c)\n",
    "\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # append the sampled character to the sequence\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # return the generated sequence\n",
    "        return idxes\n",
    "\n",
    "\n",
    "    def predict(self, data_generator, start, n):\n",
    "        \"\"\"\n",
    "        Generate a sequence of n characters using the trained LSTM model, starting from the given start sequence.\n",
    "\n",
    "        Args:\n",
    "        - data_generator: an instance of DataGenerator\n",
    "        - start: a string containing the start sequence\n",
    "        - n: an integer indicating the length of the generated sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "        # initialize input sequence\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        # initialize cell state and hidden state\n",
    "        a = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "            \n",
    "        # generate new sequence of characters\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "            i = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "            f = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "            cc = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "            c = f * c + i * cc\n",
    "            o = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "            a = o * np.tanh(c)\n",
    "            # compute the output probabilities\n",
    "            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        \n",
    "        txt = ''.join(data_generator.idx_to_char[i] for i in idxes)\n",
    "        txt.replace('\\n',\"\")\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator('text.txt', sequence_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!uLdkGHPjexLWg;WHkFy?y?ErgWxZyghAwKBfP? $sYADjAHdq:dmdsPK?!XF-g-uYUHEuegCRAV$vZovUC3hNJOzaEKiDYO.Cx&z;Lq-N3ZAR;dyzzJYpyKKtwLHwPr&m!KBV .IXg?wW C&&!eNySEVHoQxFTk?NUZnX'j&CKbpt&?'FTA:RU?Z\n",
      "L!Rj\n",
      "?GkXtOZE \n",
      "\n",
      "\n",
      "iter :0, loss:104.359684\n",
      "hl::f\n",
      "\n",
      "phoC-:snO: wy mrume!aIsshs!\n",
      "a;t Wkn wcdil menm-ashnbsg\n",
      "dnC,hsVnf.e\n",
      "euI  yvnnr tovue. ouced ;ocou,ei,flTtTe\n",
      "IAAwInthas, se\n",
      "beIEOad !tgaeuT\n",
      "bs uhawen or\n",
      "refautkatIs tzA kt eos\n",
      "if myrs t. .eaC,\n",
      "an\n",
      "\n",
      "\n",
      "iter :1000, loss:86.693235\n",
      " seas fa sollbt-eev, tCarfiard\n",
      "so wlisl wne h-me hadillthTh, phar pevene ty btrar feiss\n",
      "ib womome thehoad cakis or\n",
      "eak areng fom themncn s af aouune ryan, iheiil se boamt\n",
      "e Imon aor\n",
      "em:\n",
      "wire pat itrhe\n",
      "\n",
      "\n",
      "iter :2000, loss:72.888867\n",
      "nt ranis, ynpael le'sgs, gosL\n",
      "A anCon tiat\n",
      "Af Iacg aad\n",
      "\n",
      "FS althe cihe'at,\n",
      "Tal woe shou;calot\n",
      "?e ,ciud lis?\n",
      "temen Iozze npehe peseuwhe hewhe gh't hem, aat yaw the biols he ptte the woure bith mime,\n",
      "\n",
      "Wo\n",
      "\n",
      "\n",
      "iter :3000, loss:65.185396\n",
      "bIUS:\n",
      "THeb, ane iy muffoul be foieins weme:\n",
      "Arny wGtn boo d bse yme, st lo c are fmurzere malc and Uded meins bon,\n",
      "I Ire pabey ghom ly bat go stee sdow? the.\n",
      "\n",
      "MOVRLAIULS:Hl un ale an:\n",
      "af loz\n",
      "nens Traa\n",
      "\n",
      "\n",
      "iter :4000, loss:60.815305\n",
      "ian hive thit melanr\n",
      "\n",
      "Finesd libe iI :ens:\n",
      "\n",
      "hEiOrIAUS:\n",
      "\n",
      "eins tomey\n",
      "uHe chceisreand. banelingman\n",
      "\n",
      "aB ieranU:\n",
      "Mithhe bxeand uisi-nou, com.\n",
      "\n",
      "WCIILIUSs\n",
      "And Aasurg bemenle yorrhe tfol irckue ph'cilhes an, \n",
      "\n",
      "\n",
      "iter :5000, loss:58.668385\n",
      "rt\n",
      "Aonpy\n",
      "I whare tmea'ld yo mem ow: klar, ars aidt ham henill ther:\n",
      "Nur I urtond im the scouthirewin sheng lour\n",
      "Ane soor\n",
      "rkey;rans het iis mo pepransm moust.\n",
      "\n",
      "SFishons neit's heacs mle whinrsin: helw \n",
      "\n",
      "\n",
      "iter :6000, loss:57.449504\n",
      "o co cher oud if doxgeblevy ervetharkn have coifr,\n",
      "Ajccvena, hy tid oud!\n",
      "Lnd Tho domekzyor fht oud.\n",
      "\n",
      "sENOL\n",
      "HETRePn\n",
      "TNuthins if toy I hravo,\n",
      "Ihe gipr, amy us the te tnood, sorkond fover!\n",
      "Ard Roughiund \n",
      "\n",
      "\n",
      "iter :7000, loss:57.250306\n",
      "ty my rond, comcu co that!\n",
      "Bree fon I pracancours bor, wy Cusceky ow?\n",
      "\n",
      "\n",
      "LORAAN Gzur:\n",
      "kams thouw, ne ind tere?\n",
      "\n",
      "GFEAjLNNES:\n",
      "Bous werced on.\n",
      "\n",
      "KELAAENC\n",
      "MDAlld:\n",
      "Lere upesr rfear.\n",
      "\n",
      "SeRcndo\n",
      "I not:\n",
      "Werll doc\n",
      "\n",
      "\n",
      "iter :8000, loss:55.999026\n",
      "ad GolsmsarntFlrowhineitt doughle ontiwors sat inrer winlennt Murhengiwher thevennt indele,\n",
      "ffor fas our, Ials you thin Iy one.\n",
      "\n",
      "QUCKENNBAW\n",
      "-ESMThe, ore thy mulp lreigs grellwenc?\n",
      "Thith y ourbidengsti\n",
      "\n",
      "\n",
      "iter :9000, loss:55.588094\n",
      "ichef\n",
      "Toerglind this my coverlessserat.\n",
      "Yhel Od The eas'mestewere thel tae,\n",
      "Thet outhAndin hous patheawimlisengfiup\n",
      "fod usnout, lawe licy hatbevinge whiad\n",
      "Migroce congrienccotheis fat, ghtavungouls:\n",
      "G\n",
      "\n",
      "\n",
      "iter :10000, loss:54.325006\n",
      "rath suecofguld buky hath merad, wfore soust,\n",
      "The rheas, latinestretel: of theve nants melds.\n",
      "Ane ald ins withce deageh of tnere.\n",
      "\n",
      "BENEN FIRINKMHin, ot wonkeserit.\n",
      "Buther afy nobrily, to erce sndoud t\n",
      "\n",
      "\n",
      "iter :11000, loss:54.324055\n",
      "ge-es'r snlech henk.\n",
      "YUTou lorser\n",
      "Saiche veaspeil, m cyors'linr tharue,\n",
      "\n",
      "inn angresgHed:\n",
      "If lind, be the worest sa.\n",
      "\n",
      "CCERCENH:\n",
      "The glownod;\n",
      "Kill so\n",
      "dy hext ais oo me.\n",
      "\n",
      "RINEC:\n",
      "Ry awtor:\n",
      "Enon ir wmeGrup\n",
      "\n",
      "\n",
      "iter :12000, loss:52.198375\n",
      "mlse our thomy.\n",
      "\n",
      "GLTOC,\n",
      "And fuit madce fuak be suromed sowlo, hy boous whallra's, I wast, beoch haf me hlow nhtheils, bedre;\n",
      "The paiges aive  the inbe souht, SerlE\n",
      "Thcein nad of cildobdy kist siy sst \n",
      "\n",
      "\n",
      "iter :13000, loss:53.494350\n",
      "ogenge not! in thiv wawis faod shate\n",
      "That that buve to uud pilify kith, Gowe thaik bith'ldonWalas's Hopald onden ande pars ipnet.\n",
      "\n",
      "QHERA OGFAF:\n",
      "Thith tug menl ond soubt; dive of efore fordd, if shes t\n",
      "\n",
      "\n",
      "iter :14000, loss:53.256031\n",
      " aliinnad bea sot\n",
      "Bngy thich al Iake nak for of me boyle.\n",
      "\n",
      "BuveO Hent:\n",
      "And you ntin hishtle pofe: acd to eot?\n",
      "\n",
      "HELH RM LOUN:\n",
      "He llond ars butht in the this o and.\n",
      "More, Noil vemcegfad rMaoce corserif \n",
      "\n",
      "\n",
      "iter :15000, loss:52.541110\n",
      "en the hirgt ne to\n",
      "-whele spat dom\n",
      "she fall forded on engrighonslsd,\n",
      "If Oukiwg whit epot wingeend. Prould,\n",
      "Mas me hom nourst what he pnott,\n",
      "Cusheny chiss herca pray, bnthigher no what afand,\n",
      "Le te nes\n",
      "\n",
      "\n",
      "iter :16000, loss:52.113087\n",
      " Gom hatple to inpe astee the bermaBp.\n",
      "\n",
      "BDEY OUdGWRIK:\n",
      "You githert tatto\n",
      "Th thy ene wele Rom'e nomere. Corking;\n",
      "To guelt Bedcmun' mung thr toll sy waid.\n",
      "And ware is wand wh ghith mens konged:\n",
      "When the\n",
      "\n",
      "\n",
      "iter :17000, loss:51.019603\n",
      "Yor nom:\n",
      "SRirghtre of sun suan ceain, coweris.\n",
      "\n",
      "GARA ELINGDUAS:\n",
      "The mud wimust urist whe hove his\n",
      ".\n",
      "\n",
      "SEODNe:\n",
      "Ant lodsthan, ouls lalt smeb,\n",
      "Magtele fertery ourK,\n",
      "And neantting heves ore the michtew\n",
      "My \n",
      "\n",
      "\n",
      "iter :18000, loss:50.971129\n",
      "s'me 'ach,\n",
      "I king wim-gowe her ha bie syex of he:\n",
      "Lot hemse hepell-s.\n",
      "This lead thik Cit lount' hos; delm pos te kewer in sotmen.\n",
      "\n",
      "DbrOIT:\n",
      "Null wis, speret and them ull Buctish,\n",
      "Af reverg hy sendither\n",
      "\n",
      "\n",
      "iter :19000, loss:52.032690\n",
      "n yswe'l, wive soll muse\n",
      "Thou gace cove thouuo wime changet nut.\n",
      "\n",
      "LoOd I Lod:\n",
      "Ay Foy theuse hounts Re harr beds oul thy sall wall aar wearpworo, there gond hous nome,\n",
      "Courst gecbes'ed high ther awline\n",
      "\n",
      "\n",
      "iter :20000, loss:51.429793\n",
      " to bleothy, hathy wild\n",
      "For of my liyert wn thither,\n",
      "Tourringc! I fooreatrore the hist lame.\n",
      "Heread no thourd, but menmyowh tmat, sirs liy.\n",
      "\n",
      "RONE:\n",
      "O friCtoderee olbay thescllstear?\n",
      "Thescfuothey misars\n",
      "\n",
      "\n",
      "iter :21000, loss:51.660104\n",
      "l theve dane; hatp, nos vardsinguebopout,\n",
      "To king, are soun illoof shat; ade I waod op,urstem,\n",
      "Ay, sheprencoung mo by geistscous'd A fatise:\n",
      "All ben larJs. quwinthe baly, thy nore'? chout ti,\n",
      "yow I my\n",
      "\n",
      "\n",
      "iter :22000, loss:50.311368\n",
      "llleane'd 'llowe, he, Yougit\n",
      "the tak hem ofer; gain soor worro, sere mo foblad in to toughoul\n",
      "Couth thered you go wa a homy kend,\n",
      "Gond Cetriam ofe theeln of for if ysusturs be man?\n",
      "Aw whou ling.\n",
      "\n",
      "uRII\n",
      "\n",
      "\n",
      "iter :23000, loss:50.541260\n",
      "f thew ald himt;\n",
      "And paryo, th yanees is thibe, tho; and Runon thied ald ufaith,\n",
      "Yark:\n",
      "So smaurn; k'rit and the with thas with pres day me thare:\n",
      "Swire neand! Ruscowe pist. Cothat ond bongy's is fot; \n",
      "\n",
      "\n",
      "iter :24000, loss:50.275475\n",
      "a.:\n",
      "'nws nik thy I palanfs sian, swell I kide tlauscel\n",
      "And well I be! her wrobk's very has iie a ligsqoring!\n",
      "\n",
      "MLICFORDH:\n",
      "For, Cither, I'll dith tord-and Ih weth;\n",
      "Not is have ppot iw nan that ard.\n",
      "\n",
      "JON\n",
      "\n",
      "\n",
      "iter :25000, loss:49.539543\n",
      "r ak sepall, to te thene.\n",
      "NRkene:\n",
      "Hercgish, ane, lid shoulven thie Shent my lathie and of,\n",
      "Whom de notice IKat ipest and his reach sich.\n",
      "\n",
      "NLEY YORD,\n",
      "Sary my, komero's maty hest a will spalis.\n",
      "Ay? Have\n",
      "\n",
      "\n",
      "iter :26000, loss:48.540841\n",
      "w, wimer my ould aw fut the larceke! wo live Burhy?\n",
      "\n",
      "RAFERIS:\n",
      "Torl Londpise the Godatrever whill,\n",
      "But tay, the hish lomd' orke's my Lomn,\n",
      "There wo puxlevigh whe and fpare of a difes?\n",
      "\n",
      "RICE RENBVV:\n",
      "Sha\n",
      "\n",
      "\n",
      "iter :27000, loss:48.254325\n",
      "y s alful de ble, my monce dearnsenf?\n",
      "Oy, and un a his puthereard.\n",
      "WKther foree to puncy wher, a kand ad, shears'd furce baks.\n",
      "SO qullevidnshallowglas make my wichy.\n",
      "And, shund iges marite what om sis\n",
      "\n",
      "\n",
      "iter :28000, loss:48.036131\n",
      "s ome nowart,\n",
      "Got rengess at urey Joum'suml seldcasus sus it mareca hond,\n",
      "That fear, Heven Intermter, peis'd aed your gooth-rech:\n",
      "But thing to noufonto, in\n",
      "O to supe ingen then smore,\n",
      "-ulinge to hop, \n",
      "\n",
      "\n",
      "iter :29000, loss:49.462586\n",
      " tat, and yor hingurte,\n",
      "sty yout love hothi-d!\n",
      "That ad hash flious\n",
      "; I the one ro scrler-h\n",
      "Twal to proth sstall aie leaver wordshor,\n",
      "Poflid e gord-.\n",
      "Or the quick in miin steiting, there shace ow ore:\n",
      "\n",
      "\n",
      "\n",
      "iter :30000, loss:49.413339\n",
      "r id hame mhear tient's my liths.\n",
      "\n",
      "DiMike:\n",
      "Shereat theme thim baisent if dose ol thee fover-shing watt il heer pangien me dwyss of tie fue conds,\n",
      "Thou than ne: shall, betion anoy, and untought ne,\n",
      "In \n",
      "\n",
      "\n",
      "iter :31000, loss:49.836002\n",
      "is. noulfors fword wish lave.\n",
      "Feopfove- of palf'tliang thes misty.\n",
      "\n",
      "ThYur:\n",
      "Yeal them to: awh of allece\n",
      "smanrred, and ghen her a vear as forsthel\n",
      "dise chemo and fard. \n",
      "Shengeank:\n",
      "O with yrurce; I laifn\n",
      "\n",
      "\n",
      "iter :32000, loss:49.840158\n",
      "is-:\n",
      "Thou tis, thece aildss, the gicks\n",
      "Nom? radoly Dood fir youry, swall of hivery.\n",
      "\n",
      "CichFrRE:\n",
      "O'd ssech was a save\n",
      "I thereed 'el's at wis.\n",
      "one seet wiy the? my but to head ald whus\n",
      "'last inr, bo thor\n",
      "\n",
      "\n",
      "iter :33000, loss:48.692626\n",
      "Coafer congeveing! ic sough lefw scames-\n",
      "And catl whith ar sartans offy.\n",
      "\n",
      "LARWE:\n",
      "Ohe I love ssoy, ofherd\n",
      "pamince. Row ore heve sones, your licght of of the.\n",
      "Weacl us tyon ugla rivearadupies's welcee '\n",
      "\n",
      "\n",
      "iter :34000, loss:48.357457\n",
      "OCXYAL:\n",
      "Bey low, I whor aft sholl's dund;\n",
      "hay you beot in ip whellith of owar not his laakelifee the gang, you, to haw?\n",
      "\n",
      "CETALO:\n",
      "Hea, hath nor hope ferly thap\n",
      "Go vould Kis Carnfy liste troo?\n",
      "Wet I wer\n",
      "\n",
      "\n",
      "iter :35000, loss:48.563887\n",
      "o ssours itccander you have with she fror l'stire\n",
      "The eathay the wind!y me not stain.\n",
      "\n",
      "MYoMELI:\n",
      "Whis bucl fid of mont!\n",
      "Arven thou tien uspell no sand is ivem my beecf\n",
      "Ippeld ne ghese nat my offrich's.\n",
      "\n",
      "\n",
      "iter :36000, loss:47.431635\n",
      " or\n",
      "And bust gowian broze?\n",
      "\n",
      "LLOUCYO:\n",
      "Micliey, soming koot not buth He.\n",
      "virt. Duty noge if how ay seltof oun not he at of that kenewour domued.\n",
      "\n",
      "PUStoRK:\n",
      "Mard if couwsuen.\n",
      "\n",
      "Peivene:\n",
      "Vcacence daky Lond \n",
      "\n",
      "\n",
      "iter :37000, loss:47.554172\n",
      " that beth the anver! Vireist.\n",
      "Or the cemes of tha howd ffiren!\n",
      "With the heve gamly frkeckitasefisus riens; not bu.\n",
      "Thy notinish s,weftinntise you, por. Gown trou dUdon;\n",
      "Wo spes tutr proce it is dome.\n",
      "\n",
      "\n",
      "iter :38000, loss:46.722687\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(hidden_size=200, vocab_size=data_generator.vocab_size, sequence_length=25, learning_rate=1e-3)\n",
    "lstm.train(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clardtras,\\nI lave vave me lotwer parpais,\\nI to bles as omd yout of lest Conole! \\nImen sibthed, dear hibry, wer you:\\nBy you grows fate comly be doid!\\n\\nD'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.predict(data_generator, \"c\", 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did for the RNN, we're tracking the output for the same input (which now will be the same for all later models - before, we had the limitation of the vocabulary instantiated in the dataset). Now, we can use the actual wording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"meme.jpg\" width=\"300\" alt=\"look for it's been 84 years on google images\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>itsbeen</td>\n",
       "      <td>itsbeenachenosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vanilla LSTM</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-foursen frow't wis calestnow:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                  Input  \\\n",
       "0           RNN                itsbeen   \n",
       "1  Vanilla LSTM  It's been eighty-four   \n",
       "\n",
       "                                              Output  \n",
       "0                                itsbeenachenosaurus  \n",
       "1  It's been eighty-foursen frow't wis calestnow:...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate output for LSTM\n",
    "lstm_output = lstm.predict(data_generator, \"It's been eighty-four\", 150)\n",
    "\n",
    "# Add LSTM results to the dataframe\n",
    "new_row = pd.DataFrame({\n",
    "    'Model': ['Vanilla LSTM'],\n",
    "    'Input': [\"It's been eighty-four\"],\n",
    "    'Output': [lstm_output]\n",
    "})\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we fed \"eighty-four\" instead of \"84\". That's because numbers are not included in the vocab, so the expected result should show hallucinations, since these inputs are clearly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells define and implement the Gated Recurrent Unit (GRU) class, another variant of RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (GRU).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the GR.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the GRU.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the GRU.\n",
    "    self.learning_rate : float\n",
    "        The learning rate used during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n",
    "        Initializes an instance of the GRU class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the GRU class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the GRU.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the GRU.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the GRU.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # model parameters\n",
    "        self.Wz = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wr = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (vocab_size, hidden_size))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "\n",
    "        # initialize gradients for each parameter\n",
    "        self.dWz, self.dWr, self.dWa, self.dWy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(\n",
    "            self.Wa), np.zeros_like(self.Wy)\n",
    "        self.dbz, self.dbr, self.dba, self.dby = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(\n",
    "            self.bz), np.zeros_like(self.by)\n",
    "\n",
    "        # initialize parameters for adamw optimizer\n",
    "        self.mWz = np.zeros_like(self.Wz)\n",
    "        self.vWz = np.zeros_like(self.Wz)\n",
    "        self.mWr = np.zeros_like(self.Wr)\n",
    "        self.vWr = np.zeros_like(self.Wr)\n",
    "        self.mWa = np.zeros_like(self.Wa)\n",
    "        self.vWa = np.zeros_like(self.Wa)\n",
    "        self.mWy = np.zeros_like(self.Wy)\n",
    "        self.vWy = np.zeros_like(self.Wy)\n",
    "        self.mbz = np.zeros_like(self.bz)\n",
    "        self.vbz = np.zeros_like(self.bz)\n",
    "        self.mbr = np.zeros_like(self.br)\n",
    "        self.vbr = np.zeros_like(self.br)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, c_prev, a_prev):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for a simple GRU model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "\n",
    "        Returns: X (numpy array): Input sequence, shape (sequence_length, input_size) c (dictionary): Cell state for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) r (dictionary): Reset gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) z (dictionary): Update gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) cc (dictionary): Candidate cell\n",
    "        state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) a (dictionary):\n",
    "        Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) y_pred (\n",
    "        dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (\n",
    "        output_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize dictionaries for backpropagation\n",
    "        # initialize dictionaries for backpropagation\n",
    "        r, z, c, cc, a, y_pred = {}, {}, {}, {}, {}, {}\n",
    "        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n",
    "        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n",
    "\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(X.shape[0]):\n",
    "            # concatenate the input and hidden state\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a[t - 1], xt))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r[t] = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z[t] = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc[t] = np.tanh(np.dot(self.Wa, np.vstack((r[t] * a[t - 1], xt))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c[t] = z[t] * cc[t] + (1 - z[t]) * c[t - 1]\n",
    "\n",
    "            # compute the hidden state\n",
    "            a[t] = c[t]\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n",
    "\n",
    "        # return the output probability vectors, cell state, hidden state and gate vectors\n",
    "        return X, r, z, c, cc, a, y_pred\n",
    "\n",
    "    def backward(self, X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets):\n",
    "        \"\"\"\n",
    "        Performs backward propagation through time for a GRU network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "            r (dictionary): Reset gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            z (dictionary): Update gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n",
    "            targets (numpy array): Target outputs for each time step, shape (sequence_length, output_size)\n",
    "\n",
    "        Returns:\n",
    "            None       \n",
    "        \"\"\"\n",
    "        # Initialize gradients for hidden state\n",
    "        dc_next = np.zeros_like(c_prev)\n",
    "        da_next = np.zeros_like(a_prev)\n",
    "\n",
    "        # Iterate backwards through time steps\n",
    "        for t in reversed(range(X.shape[0])):\n",
    "            # compute the gradient of the output probability vector\n",
    "            dy = np.copy(y_pred[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # compute the gradient of the output layer weights and biases\n",
    "            self.dWy += np.dot(dy, a[t].T)\n",
    "            self.dby += dy\n",
    "\n",
    "            # compute the gradient of the hidden state\n",
    "            da = np.dot(self.Wy.T, dy) + da_next\n",
    "\n",
    "            # compute the gradient of the update gate\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a_prev, xt))\n",
    "            dz = da * (a[t] - c[t])\n",
    "            self.dWz += np.dot(dz, concat.T)\n",
    "            self.dbz += dz\n",
    "\n",
    "            # compute the gradient of the reset gate\n",
    "            dr = da * np.dot(self.Wz[:, :self.hidden_size].T, dz) * (1 - r[t]) * r[t]\n",
    "            self.dWr += np.dot(dr, concat.T)\n",
    "            self.dbr += dr\n",
    "\n",
    "            # compute the gradient of the current hidden state\n",
    "            da = np.dot(self.Wa[:, :self.hidden_size].T, dr) + np.dot(self.Wz[:, :self.hidden_size].T, dz)\n",
    "            self.dWa += np.dot(da * (1 - a[t]**2), concat.T)\n",
    "            self.dba += da * (1 - a[t]**2)\n",
    "\n",
    "            # compute the gradient of the input to the next hidden state\n",
    "            da_next = np.dot(self.Wr[:, :self.hidden_size].T, dr) \\\n",
    "                      + np.dot(self.Wz[:, :self.hidden_size].T, dz) \\\n",
    "                      + np.dot(self.Wa[:, :self.hidden_size].T, da)\n",
    "        # clip gradients to avoid exploding gradients\n",
    "        for grad in [self.dWz, self.dWr, self.dWa, self.dWy, self.dbz, self.dbr, self.dba, self.dby]:\n",
    "            np.clip(grad, -1, 1)\n",
    "\n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # calculate cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
    "\n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the GRU's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # AdamW update for Wz\n",
    "        self.mWz = beta1 * self.mWz + (1 - beta1) * self.dWz\n",
    "        self.vWz = beta2 * self.vWz + (1 - beta2) * np.square(self.dWz)\n",
    "        m_hat = self.mWz / (1 - beta1)\n",
    "        v_hat = self.vWz / (1 - beta2)\n",
    "        self.Wz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wz)\n",
    "\n",
    "        # AdamW update for bu\n",
    "        self.mbz = beta1 * self.mbz + (1 - beta1) * self.dbz\n",
    "        self.vbz = beta2 * self.vbz + (1 - beta2) * np.square(self.dbz)\n",
    "        m_hat = self.mbz / (1 - beta1)\n",
    "        v_hat = self.vbz / (1 - beta2)\n",
    "        self.bz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bz)\n",
    "\n",
    "        # AdamW update for Wr\n",
    "        self.mWr = beta1 * self.mWr + (1 - beta1) * self.dWr\n",
    "        self.vWr = beta2 * self.vWr + (1 - beta2) * np.square(self.dWr)\n",
    "        m_hat = self.mWr / (1 - beta1)\n",
    "        v_hat = self.vWr / (1 - beta2)\n",
    "        self.Wr -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wr)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mbr = beta1 * self.mbr + (1 - beta1) * self.dbr\n",
    "        self.vbr = beta2 * self.vbr + (1 - beta2) * np.square(self.dbr)\n",
    "        m_hat = self.mbr / (1 - beta1)\n",
    "        v_hat = self.vbr / (1 - beta2)\n",
    "        self.br -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.br)\n",
    "\n",
    "        # AdamW update for Wa\n",
    "        self.mWa = beta1 * self.mWa + (1 - beta1) * self.dWa\n",
    "        self.vWa = beta2 * self.vWa + (1 - beta2) * np.square(self.dWa)\n",
    "        m_hat = self.mWa / (1 - beta1)\n",
    "        v_hat = self.vWa / (1 - beta2)\n",
    "        self.Wa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wa)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for Wy\n",
    "        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n",
    "        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n",
    "        m_hat = self.mWy / (1 - beta1)\n",
    "        v_hat = self.vWy / (1 - beta2)\n",
    "        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "        m_hat = self.mby / (1 - beta1)\n",
    "        v_hat = self.vby / (1 - beta2)\n",
    "        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n",
    "\n",
    "    def train(self, data_generator,iterations):\n",
    "        \"\"\"\n",
    "        Train the GRU on a dataset using backpropagation through time.\n",
    "\n",
    "        Args:\n",
    "            data_generator: An instance of DataGenerator containing the training data.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        iter_num = 0\n",
    "        # stopping criterion for training\n",
    "        threshold = 50\n",
    "    \n",
    "        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "        while (iter_num < iterations):\n",
    "            # initialize hidden state at the beginning of each sequence\n",
    "            if data_generator.pointer == 0:\n",
    "                c_prev = np.zeros((self.hidden_size, 1))\n",
    "                a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = data_generator.next_batch()\n",
    "\n",
    "            # forward pass\n",
    "            X, r, z, c, cc, a, y_pred = self.forward(inputs, c_prev, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[self.sequence_length - 1]\n",
    "            c_prev = c[self.sequence_length - 1]\n",
    "#             if iter_num == 5900 or iter_num == 30000:\n",
    "#                         self.learning_rate *= 0.1\n",
    "            # print progress every 100 iterations\n",
    "            if iter_num % 100 == 0:\n",
    "#                 self.learning_rate *= 0.99\n",
    "                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n",
    "                print(''.join(data_generator.idx_to_char[idx] for idx in sample_idx))\n",
    "                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
    "            iter_num += 1\n",
    "\n",
    "    def sample(self, c_prev, a_prev, seed_idx, n):\n",
    "        \"\"\"\n",
    "        Sample a sequence of integers from the model.\n",
    "\n",
    "        Args:\n",
    "            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n",
    "            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n",
    "            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n",
    "            n (int): Number of characters to generate.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize input and seed_idx\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        # convert one-hot encoding to integer index\n",
    "        seed_idx = np.argmax(seed_idx, axis=-1)\n",
    "\n",
    "        # set the seed letter as the input for the first time step\n",
    "        x[seed_idx] = 1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        idxes = []\n",
    "        c = np.copy(c_prev)\n",
    "        a = np.copy(a_prev)\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "            c = z * c + (1 - z) * cc\n",
    "            a = c\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # append the sampled character to the sequence\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # return the generated sequence\n",
    "        return idxes\n",
    "\n",
    "    def predict(self, data_generator, start, n):\n",
    "        \"\"\"\n",
    "        Generate a sequence of n characters using the trained GRU model, starting from the given start sequence.\n",
    "\n",
    "        Args:\n",
    "        - data_generator: an instance of DataGenerator\n",
    "        - start: a string containing the start sequence\n",
    "        - n: an integer indicating the length of the generated sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "        # initialize input sequence\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        # initialize cell state and hidden state\n",
    "        a = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # generate new sequence of characters\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c = z * cc + (1 - z) * c\n",
    "\n",
    "            # compute the hidden state\n",
    "            a = c\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        txt = ''.join(data_generator.idx_to_char[i] for i in idxes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F&itLs$&3frB\n",
      "W;xUhka'QoXOgtND'sqd&qkqVd$Kgq..UJrm$U,aaLt:n:b kHUCF:lHddjg'dchb3ykagaahMHBufPzVEWq-fGytX:oW'iCCauaTD?AyXlun'TdLN!3Wj$eqHB'u\n",
      "F:GYHAQDo?SPiIp U'U;BE.D:GVzKb!-lbUuldEURXL.UH!rClq-zbL ibtzC\n",
      "\n",
      "\n",
      "iter :0, loss:100.185091\n",
      "ros Joeny \n",
      "ou tiiedh-Qcotih h aHXi\n",
      "l\n",
      "Xthinthhaknio en tg hiethgl. a?N iestthFFrrlae\n",
      "utheezk\n",
      "TisdsoouthnhGnxw\n",
      "l\n",
      "\n",
      "\n",
      "SqrY h tiXsouerlnndth sofasakaoutoshyom'i thdt hiinsuABu\n",
      "ziKstistOioMfuiO&dtho aatChizh\n",
      "\n",
      "\n",
      "iter :100, loss:98.310289\n",
      " aens\n",
      " mhe nt atens ar fste nmto h ataeith euds omuite yre rcon wwo o arre alyums t  mouspyo em o ay wt os st afngs l amatyin tinrels to\n",
      "uo nodG eth e eae\n",
      "rnto e ator st t atiuatMas,t inrgeeso uuise t\n",
      "\n",
      "\n",
      "iter :200, loss:95.633728\n",
      "amerre  fhared3 ayot he wthe me allll\n",
      "et bmong ate ate  pol b our\n",
      "\n",
      "Wnse,t os, g ot eres afgl:l\n",
      "n ther wnse d udt awit m ellld:y, foduto my,e bllvl ve aitaec ahaithed e mynoi\n",
      "n\n",
      "\n",
      "Fleld fmr av\n",
      "A  mirt ha\n",
      "\n",
      "\n",
      "iter :300, loss:92.893081\n",
      "mn, akouop oufr dev istrer abmo bndy,ou, ate oue cathe, pomou\n",
      "l\n",
      "\n",
      "FYrs, be pyofus,f\n",
      "Oeli ofind wond t off.and,aeccrnou math e boordel yo'dyor ti hea w llyo pelis aktalivthre,s, wfd poug,\n",
      "\n",
      "FEsece adb,pr\n",
      "\n",
      "\n",
      "iter :400, loss:90.536032\n",
      "r\n",
      "sa t her, ants wanpod y thecond wo hecoandecs's tit her,\n",
      "Whl y pyouantyin wthe yuous, pygiinngr watheran nourt herthaen s woug\n",
      "\n",
      "A,ourans catinoth \n",
      "Wheeandd shu ours\n",
      "\n",
      "Thes orerd mesand yhe'ouds, m we\n",
      "\n",
      "\n",
      "iter :500, loss:88.582884\n",
      " wis wis,.\n",
      "TThhe min pno'ru tiond sther thaud rierins wingrant\n",
      "Thaer byouo hate ts ofwak it titen se:ran:\n",
      "Mns oandi ongns pu\n",
      "MIUSs ant haen' nootofarte eno p'p\n",
      "T ome:\n",
      "ThaIrU m thoir tyincr atheir atni\n",
      "\n",
      "\n",
      "iter :600, loss:86.491654\n",
      "OlIS:\n",
      "\n",
      "Thin:\n",
      "\n",
      "Fice cen:\n",
      "Sin:\n",
      "Whiouh, fd han fidt hono. di, hobem dingd\n",
      "orourd.\n",
      "M hive'di minsd Cinens. \n",
      "I Iding tto' rtsu ankon d yusMon mint\n",
      "SENIENUcNI:\n",
      "MIENNIStho  wpheeverms sommaitnd o't hatoiznen\n",
      "\n",
      "\n",
      "iter :700, loss:84.988305\n",
      "nds larant tir C the amere afrede sacl tonhe:\n",
      "MEd,hee\n",
      "ivlld yl'd ggn:e\n",
      "p eruee ymaurt ate hcar at,h alk oof\n",
      "An'ear atind atheer.\n",
      "Afo:S:\n",
      "OIU\n",
      "Tofreme tand is arrd.\n",
      "Tho kffo.\n",
      "S\n",
      "Md.\n",
      "AHono we\n",
      "ans mean:\n",
      "TWo\n",
      "\n",
      "\n",
      "iter :800, loss:83.729315\n",
      "m Canko wil ihme dd marel ate welelar fa ricencomke. Ipe f worm o'\n",
      "uur; ha I Inooul whebi dad hadho boum atst. han hit yl rad feme wId ofee yowos eard emad hak mate emef bfe deI\n",
      "S:Il ebe b' ay st\n",
      "he l\n",
      "\n",
      "\n",
      "iter :900, loss:81.950683\n",
      "\n",
      "kOeild oell hak wea rl as rel------\n",
      "ENI Itheor then hande o dtr ouly, him y ous th ofo fowrhut ofrey powoulo\n",
      "AA:hamo, Imerea glll dy,of hgalugs.\n",
      "YVIUS:\n",
      "Gl;hof rt yowley lll swhees;\n",
      "IArl:\n",
      "ARCerl thowe\n",
      "\n",
      "\n",
      "iter :1000, loss:80.337213\n",
      " ouur's!e oow.rsuspr t ghol ullak, shigmon,ully-hid isigthor ld oul fouto\n",
      "O\n",
      "OIluININIUS:\n",
      "OMENINI Ifris;\n",
      "\n",
      "OAARouIS:\n",
      "MENIhis ththor, vey tr ous ust or bgat fe weer:\n",
      "RIUS:\n",
      "HVICi then, i gkontof orthe yo \n",
      "\n",
      "\n",
      "iter :1100, loss:78.715892\n",
      "rt, Ile;s ha tpoar te' tiss;!es suls:\n",
      "\n",
      "BoRes hes, halk itho fomen, currcikorut\n",
      "hit, bl orupot sous then ghilbalAur,e ty' werstth,\n",
      "izARCrebte,\n",
      "winor cesas!out thot, acnh, gath isourst outsi hart, tou s\n",
      "\n",
      "\n",
      "iter :1200, loss:77.160119\n",
      " woug bindithera tyir, n the ser paes us theercaus athes! now\n",
      "Hour t enfcreus\n",
      "LIARu ganor, the ts surise,\n",
      "\n",
      "OCerit arc t olond, sh aurs lapAer imese, nitse ius fon tes us, mand, uh onugsurcen;; Iord gt\n",
      "\n",
      "\n",
      "iter :1300, loss:75.578255\n",
      "S:\n",
      "HCIARe fisncon\n",
      "W Caiuke car owey fonckon sthiv amatseus,\n",
      "WH'\n",
      "CIUS:o\n",
      "\n",
      "Core pouthits hader,\n",
      "The anomw amt as ci friul, dous dasi ngd an the and in owhis am tserce, thi f ericke ise\n",
      "ff avene ty ono wi\n",
      "\n",
      "\n",
      "iter :1400, loss:74.476355\n",
      "sp onont et yo whan' tothen amed!etd pime they odeman wes\n",
      "Thes aton ontr--------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "iter :1500, loss:73.395542\n",
      "ede whale n thay y, t\n",
      "Whar aileme onges wwien toled allik:\n",
      "IULRes w---\n",
      "Thou\n",
      "Malod.\n",
      "\n",
      "Bo the we tye.\n",
      "BVRIU:\n",
      "Thearce iperithine  torve fe fet our ongive ilf ame inds! peoritrporen wigermey ourtt\n",
      "Fet ant \n",
      "\n",
      "\n",
      "iter :1600, loss:72.424872\n",
      "d bef tasat 'clat yo bourat.\n",
      "\n",
      "VChey glo,\n",
      "Wm yo touck he st aved ofrdfir eold yi come;\n",
      "Vcorhomal lod theimer?\n",
      "obleircil.\n",
      "\n",
      "Vhour bouk ty o ame yort omem!es!\n",
      "LOeI me myouro bok ourelds,\n",
      "\n",
      "VFe mey ow ohery\n",
      "\n",
      "\n",
      "iter :1700, loss:71.624233\n",
      "d wintes couthal pallillifet\n",
      "ARIUS: thin sous he onter.\n",
      "\n",
      "MThain grout you nginddide amy\n",
      "Me igraefr bleren owballlef omy youthe me creime amsirclive meras cingres badite\n",
      "Wh yo tporeve\n",
      "ARi\n",
      "VMIUS:\n",
      "Mame w\n",
      "\n",
      "\n",
      "iter :1800, loss:70.857535\n",
      "\n",
      "Caind yo blol.\n",
      "Ma'd.\n",
      "TpMasers atlll.\n",
      "\n",
      "Fitber.\n",
      "\n",
      "V\n",
      "LALAR IUSthe warsiatncmy ollll.\n",
      "B'ous souret lollel lnd y, hamy ourgor.\n",
      "\n",
      "Man willls.\n",
      "VLsat th anser.e\n",
      "\n",
      "VOLIAll.\n",
      "MENIARCoullirespenw atthe tha thyou.\n",
      "M\n",
      "\n",
      "\n",
      "iter :1900, loss:70.175967\n",
      ", ng hbersis chatm.\n",
      "\n",
      "VOLRCIUS:\n",
      "MUS:\n",
      "The se theret.\n",
      "\n",
      "MRIUS:\n",
      "Obouno\n",
      "Thin h aledsother hveepre hes he' was anAs MARIUS:\n",
      "BUS:\n",
      "\n",
      "The\n",
      "Yser ncke s heno warifes, wan cis, yotU she res w\n",
      "Meoth los!allpTok shu,s\n",
      "\n",
      "\n",
      "iter :2000, loss:69.373738\n",
      "plalits whiut.\n",
      "OLIUS:\n",
      "OLRILUS:\n",
      "hein, welilll:\n",
      "Then, ancinusts osan ckRIUS: thee, 'dso hwes, bulids, pove thenot blalalk ly on wids, adtas hencsuh courpres, has ar cusirtherofo wheens ithel.\n",
      "Thiulls\n",
      "VO\n",
      "\n",
      "\n",
      "iter :2100, loss:68.686928\n",
      "omes?\n",
      "\n",
      "OMENENENEENENENENINENEENENIA: dsos th' thet! end ous t hinou sthe ndof her nod arcan, ke oll k'so plolols ena, trined ars cnod cer no thedond t meo no tickes, no\n",
      "poul' spordiet hep rarent ofe e\n",
      "\n",
      "\n",
      "iter :2200, loss:68.284468\n",
      "theco fo t\n",
      "wol\n",
      "of hay of matis kere nol dded, hare ndove re'ts ofre t\n",
      "on.\n",
      "\n",
      "OLIUUS:\n",
      "CIACOe fo s onok erour des\n",
      "ow\n",
      "Ohof cal etare tono ende,\n",
      "\n",
      "BO ManTor\n",
      " tore nded ous travend; do harpime ko thomime thom\n",
      "\n",
      "\n",
      "iter :2300, loss:67.400620\n",
      " verecheed,\n",
      "Tho weh an\n",
      "BUS:\n",
      "YO'e, nso ds th gandit mithe veusl:\n",
      "\n",
      "Th ar ne de ceand ibe nto: wceto sto: t heditir ices et whe d alice k iuceli fugve edsodiveand thitel.\n",
      "OLr! nowren taf it ar ithifl acn\n",
      "\n",
      "\n",
      "iter :2400, loss:67.003600\n",
      "ored heve tore fals sinous, t fo did orindst\n",
      "Mare ngt heind: sa tcom, thearive dimingo: nd beror urd ere ce hemnitt; ak nove rimsund\n",
      "y, now, cars engon, whegive to\n",
      "ndere anw arve te Idis, ot f ireth a\n",
      "\n",
      "\n",
      "iter :2500, loss:66.307052\n",
      "non digrvugseve aved soureng grve dititizneg:\n",
      "CII' sul to:\n",
      "Th I IA: IA: nghghe mhist:h\n",
      "IOLA: noove try; igs herand,  wakien gornd diruchteveigf aryous hwancitrico rimuhs trigufid, mhet inger to laditu\n",
      "\n",
      "\n",
      "iter :2600, loss:65.834270\n",
      "vve fangditi ousce atims, buneth;\n",
      "TUS:\n",
      "e hibeice gourve gingont; Rithicurad s hecoind irve\n",
      "at keicear'thiered, veo figry; owour:\n",
      "COegon tor vehitigh  aith miu, se\n",
      "ore himy, and bavne, anwe then d owla\n",
      "\n",
      "\n",
      "iter :2700, loss:65.415448\n",
      "o hyol at he woruflad youmugne, h owhenone,r he\n",
      "igr:\n",
      "e antor ihmepexarsous huse cuch oureave\n",
      "atti; acisus?\n",
      "\n",
      "CINUI least heatthe\n",
      "\n",
      "Yon.\n",
      "CI IANINUS: abeno, mbuecond ore omurp eaveving;\n",
      "S:\n",
      "IA:\n",
      "Dbgaryoug h\n",
      "\n",
      "\n",
      "iter :2800, loss:64.981915\n",
      " dar, she cos, the\n",
      "icecousi tha my, has thoma ury oulce.\n",
      "\n",
      "Seos, sh: wo sad\n",
      "obure'd,\n",
      "Wh youraldissis henghaicl sen on cuhanus s hou hece many,\n",
      "Whu poan,  theche?\n",
      "\n",
      "ENENUS:\n",
      "Wechicon t oundt?\n",
      "\n",
      "ENI and ste\n",
      "\n",
      "\n",
      "iter :2900, loss:64.153611\n",
      "om eptothtize\n",
      "en the;\n",
      "\n",
      "Se.\n",
      "COLANUS:,\n",
      "W\n",
      "Ssh aly, she o'ser:\n",
      "Wpe,\n",
      "Sen:\n",
      "wesit\n",
      "See\n",
      "cehatcel Whany\n",
      "h paly, schus\n",
      "e dasuthar chse usen.\n",
      "\n",
      "BRIGTh se myop.\n",
      "The.\n",
      "I IA:vI haf yout Is yal s thites\n",
      "irke':\n",
      "Sel.\n",
      "\n",
      "CI\n",
      "\n",
      "\n",
      "iter :3000, loss:63.887808\n",
      "eter natt\n",
      "ant hay,\n",
      "S: uhimo ploblast ant\n",
      "epary?\n",
      "anUS:\n",
      "Se; yout an sese.\n",
      "\n",
      "HNUS:\n",
      "Tan utre\n",
      "yo bemas wones theer onte?\n",
      "\n",
      "Thaud shaur sany?\n",
      "S,\n",
      "The Imealse.\n",
      "Thi\n",
      "Se:\n",
      "Se\n",
      "INADnse,\n",
      "Ste etbere.\n",
      "\n",
      "Satre\n",
      "TUS:\n",
      "\n",
      "Ser.\n",
      "\n",
      "\n",
      "\n",
      "iter :3100, loss:63.647109\n",
      "tratizen's:\n",
      "To iply, your\n",
      "tr:ase.\n",
      "\n",
      "Ses thath ses',\n",
      "Tay, Wh thir tantwr, Yforand et for ye hplalld potthr bayu tar thay opllke,\n",
      "Ster, ady,\n",
      "The ad opretrathes.\n",
      "Ss noditre teres' tar fyo the hem.\n",
      "T ith e\n",
      "\n",
      "\n",
      "iter :3200, loss:63.479007\n",
      "eplil pter: palld tany sure batre, Wh l ste plelld,\n",
      "W hatre teen.\n",
      "wen tirel yo witlell, wlle t ongrat sthe toro ptize asthatr by tome ryo younoncer afrfo pireter:\n",
      "RTUTUS:\n",
      "Thatt there sthe no tre po to\n",
      "\n",
      "\n",
      "iter :3300, loss:63.165552\n",
      " w, flo whe piopllly beurde frelo wirint Mob at wiry.\n",
      "\n",
      "COROLANUS:\n",
      "He nt wion'd ay, wsald weliththo wil'ldt mope, nos cilldd\n",
      "The piret me my or wiellbd fount t woris,\n",
      "Tho mpalld, wirf oirrbe ticosu sic\n",
      "\n",
      "\n",
      "iter :3400, loss:62.796815\n",
      " eibgneren beoll,\n",
      "We'e orwo dincons hs Cof ofar ith oun th ave deant ogre dy, opllidsontre n'l'd avigonobut tcaAno tear war, add- thadd ono, I of oburesen tatre' ld of tean,\n",
      "\n",
      "BRUTUS:\n",
      "I CORUTOLUS:\n",
      "Se\n",
      "T\n",
      "\n",
      "\n",
      "iter :3500, loss:62.533402\n",
      "a tower,\n",
      " CIOLANUS:\n",
      "Whe corin thece fo'lsd ut ithe may, ibuld,\n",
      "\n",
      "RCIOOLA:\n",
      "fo tgo tod fo fe ry mirat\n",
      "der.\n",
      "\n",
      "BRUTUS:\n",
      "Ppe, at cofr irithag-ot fal'dd\n",
      "CIan iclist,\n",
      "\n",
      "PICO IwOLANUS:\n",
      "To bcoud thize.\n",
      "\n",
      "PI t the c\n",
      "\n",
      "\n",
      "iter :3600, loss:62.185935\n",
      "ofo, cealde nag wi\n",
      "eser iffize tmeische mak toute soth adic.\n",
      "ICOLANUS: Id adi thins matthe touns de misth?\n",
      "e\n",
      "er,\n",
      "acicENUT\n",
      "Sese gthe mu de towe courend\n",
      "e,---the aritme.\n",
      "\n",
      "ne iscerpithise nout.\n",
      "\n",
      "Pe ther \n",
      "\n",
      "\n",
      "iter :3700, loss:62.165213\n",
      "LUS:\n",
      "IDI CI OLANUS,I\n",
      "GI tizens,\n",
      "\n",
      "Aneos, Ydor edso pthir.\n",
      "\n",
      "CIOLANUS: Carunsthe menan ticeagns.\n",
      "\n",
      "CIOL ouren ysulllk' dtore ond cenaruteg;I\n",
      "Dt the thag\n",
      "I Mat the ate mat\n",
      "RTUTUS:\n",
      "IWe ing berat theghathe o\n",
      "\n",
      "\n",
      "iter :3800, loss:62.076803\n",
      "wrod, bou tat thes al doche!\n",
      "\n",
      "Fitl thengis.\n",
      "I Yut hawt hatrichins th the ma.\n",
      "\n",
      "MENEIOLANUSI:\n",
      "W\n",
      "EROIOLI I thod mouneses;\n",
      "\n",
      "RIOLANUUSI Anas weran man youre,\n",
      "W\n",
      "Tho cher gars then thicos them; warth.\n",
      "\n",
      "\n",
      "Frea\n",
      "\n",
      "\n",
      "iter :3900, loss:61.760091\n",
      "\n",
      "\n",
      "MEEit rithes shir ois,  the rveangd stho gdrigthoiss rvee nar nod iveiorco, ny!\n",
      "I CIOLAnUTher chomounst haw.\n",
      "\n",
      "Fre hespithes,\n",
      "\n",
      "MENII IOLANUS:\n",
      "\n",
      "MENIANIUS:\n",
      "\n",
      "Thes thowo th mimm.\n",
      "HCOOLUMANII CIOLANInNI w\n",
      "\n",
      "\n",
      "iter :4000, loss:61.667409\n",
      "s h omesl thes viense fre ther.\n",
      "\n",
      "CORIOLANIUTUS:\n",
      "Yo whan t he rpeqldorwe,\n",
      "BRUTUS:\n",
      "ICOLIOLNI wth lo' se chm war!\n",
      "o, we inds\n",
      "ocshe forve raves he prasen thor coches;\n",
      "MENENIUS:\n",
      "Wha st-ens\n",
      "oto chasl ate mr\n",
      "\n",
      "\n",
      "iter :4100, loss:61.773664\n",
      " vom be'l parle!\n",
      "\n",
      "MENI I whe I Hjeas atfol werd.\n",
      "\n",
      "MEEENIUS:\n",
      "Wheche peepotor mhound.\n",
      "\n",
      "MENEI nwe there ppo onse ningot chmin olll' the y oin's werove poun bobe keqe nyo Is icohe mom beag Ifrsoures, w ho\n",
      "\n",
      "\n",
      "iter :4200, loss:61.529348\n",
      "ng tweo ppealo the ppilllom bloft ad folisolll pllome dond esl'd yo omo wolot thee rowehos si\n",
      "mthes\n",
      "on!\n",
      "MENIUS: Yooul, n wi herellcetse ofo ye walllcee nom terd ood yo imfes, ant thanow, in wand athm.\n",
      "\n",
      "\n",
      "iter :4300, loss:61.699611\n",
      "t,\n",
      "Thea t atounddede; haso' tdo, not omm bekeqr, jat yous.\n",
      "MENEENI ICO LANUS:t\n",
      "NI CIOLNUS:Hes peervi viing the veitoousllekene ty\n",
      "o fart aled.\n",
      "\n",
      "MENIUS:' y owe, avean oveeqt yof olllmy ot yowhear veolk\n",
      "\n",
      "\n",
      "iter :4400, loss:61.687782\n",
      "Ales apeat nwerd,\n",
      "BRUTUThe yu parind yo' hanse seleds:\n",
      "Heesand aderepofrat anidithy! se,\n",
      "Anot d ivi yele yo'lld, birto imbes, areliss: Henes yo gofilll.\n",
      "B\n",
      "Dtou.\n",
      "\n",
      "BRUTUS:\n",
      "Ftot enele fomteth, fol dey, a\n",
      "\n",
      "\n",
      "iter :4500, loss:61.652260\n",
      "imptroittti gy tloll gangint wolet\n",
      "y IOLANUTUS:\n",
      "ANIUS:\n",
      "COLOLANUS:\n",
      "CROMINIUS:\n",
      "noled\n",
      "og.\n",
      "CICOLNUS:\n",
      "Weankt gsorum. mat isek, tanw\n",
      "y yom ey ofrom y itourgvie,an gomeke, iniderexot ty\n",
      "ye' sleitithy iands t\n",
      "\n",
      "\n",
      "iter :4600, loss:61.755181\n",
      "o.\n",
      "BRUTUS:\n",
      "Werat thot The 'sthe;\n",
      "\n",
      "BROLUTUS:\n",
      "CIOLUNUS:\n",
      "ThUTIUS:\n",
      "CIOLANUS:\n",
      "CIOLRUThan tilt at y hat hy and?\n",
      "Tow hirveex, n' tugeri tiss.\n",
      "\n",
      "Fi ICatinvi yous helat hy jity seinveeritt ayus r aveen vere, ng\n",
      "\n",
      "\n",
      "iter :4700, loss:61.871571\n",
      "oworangve 'd.\n",
      "\n",
      "COROL If om atring sthetr hand ommter:\n",
      "Theang ny,\n",
      "A\n",
      "CIUS:\n",
      "Marviig hands ang by ans inim gofur: thes bervoif ilt hsod anours isthe?\n",
      "\n",
      "Cirifingrot. hatld\n",
      "Andy elin yu berhoun!\n",
      "\n",
      "COROLOLNUS:\n",
      "\n",
      "\n",
      "iter :4800, loss:61.951148\n",
      "band ushast, nmous: Heardngowhy si ingm and he shar sot hend bertor chaniont y' this har ener by her serdint ghs na werout\n",
      "The ror evind it herve gh yowr chand\n",
      "Wel nt che' d frasin ghanaved brourdvehe\n",
      "\n",
      "\n",
      "iter :4900, loss:61.837472\n",
      "wer hsor haswrid\n",
      "Thy taners ches ldsu band ous he s basurs angre the shis er,\n",
      "The thi n owerhech.\n",
      "\n",
      "Sin ghocem; hasinggher ned\n",
      "Thin sthan sperinl thne che 'd weaste rsind\n",
      "The sharve ome berke hose rme \n",
      "\n",
      "\n",
      "iter :5000, loss:61.757913\n",
      "not, sther:\n",
      "\n",
      "Whe and by oerowsarik's thise, sderavinugrn:\n",
      "Wh sile have wern eur the' sd oowir woun I'  hawthees Sofr hend ours hcir lacns che I have whe mel.\n",
      "SI I wer aches hpoe pours aceloke', 's sal\n",
      "\n",
      "\n",
      "iter :5100, loss:61.730297\n",
      "e sor were wanese hand\n",
      "Se:\n",
      "WeTher acawn:\n",
      "NS:\n",
      "Wel hacthe\n",
      "rcs habe swel, '' hamme the\n",
      "Sre edne wivigan srelen\n",
      "\n",
      "or RThime:\n",
      "Whes isrs adno,\n",
      "Ane ce cous heswan thme tloke, thi\n",
      "Thaingarme, wour halm and hsu\n",
      "\n",
      "\n",
      "iter :5200, loss:61.732044\n",
      "e su hestr se Iatl coune nged oufro thee soflcos seolfo che turs hell pecko tho'd the s his alde wa beads:\n",
      "We hasim, anouled hid cham at anelce fes ofe olowa thet, hiken, fo hcacoko thu salid Icoersha\n",
      "\n",
      "\n",
      "iter :5300, loss:61.688356\n",
      "oonve allam,----n:\n",
      "Whil hweil, hact hin\n",
      "om uthe, I' teb alls fofcef\n",
      "a towe tha\n",
      "eneh:\n",
      "\n",
      "Scofo tell; I' thane and wrhed odt oirse elf atil irmpeelpail set ter.\n",
      "\n",
      "Ase llom tat pote,\n",
      "Burid h asthalake od oo\n",
      "\n",
      "\n",
      "iter :5400, loss:61.555870\n",
      "outr odn----pirse;\n",
      "WintThin:\n",
      "Your, hane thin dot. bor thigom ade epallpoind ono mewocok't\n",
      "NISI:\n",
      "AUS:\n",
      "Tono the\n",
      "Se,\n",
      "\n",
      "MENIINIUS:\n",
      "Youct ad ofo taten?\n",
      "\n",
      "Bsoinn:\n",
      "Yout.\n",
      "\n",
      "S,\n",
      "SI: Ia trve ano fature.\n",
      "\n",
      "Yoe, Sco;\n",
      "\n",
      "\n",
      "\n",
      "iter :5500, loss:61.661249\n",
      "porot ithsem;\n",
      "as'dl foew\n",
      "ar ithy;\n",
      "Havene.\n",
      "\n",
      "SCirteary ofal u thiimu Rofe im, this totrel; t Seno.\n",
      "\n",
      "BRUTUS:\n",
      "Hand tiripsild, thed ofouf yof thete tours thosprecot hiram, as team tain, Sofey\n",
      "of toon; thie\n",
      "\n",
      "\n",
      "iter :5600, loss:61.827218\n",
      "o trint the tobe tonetin\n",
      "Whithest eno levee tuf lofl; him inma deseste ftoefl pecore; onitht oper offo, on fithy.\n",
      "\n",
      "SICI I's ttory ed offe fifothy isatl fof aru thite.\n",
      "Thaig angmke.\n",
      "\n",
      "Af Mad SIfalfoor t\n",
      "\n",
      "\n",
      "iter :5700, loss:62.236103\n",
      "himong atr'sul; I ocond trewass ton't\n",
      "at uppref thy out foings theth irmen tad od ondecorun thenonuf ut ong noman; the hsi ald Senvent\n",
      "y, I the nat theevi's igrast odfi makn my ore. lffize nout hthist\n",
      "\n",
      "\n",
      "iter :5800, loss:62.276077\n",
      "is rme.\n",
      "Yus ofor ant tno menet sofr esa toreeve,\n",
      "otre soned htesr somenat geaveas urs\n",
      "ormemint thad menacen, Sishy areve omad, the menecan\n",
      "A nenatr Ro myu bu there mnersen: tald maen ter ble\n",
      "rsy\n",
      "sours\n",
      "\n",
      "\n",
      "iter :5900, loss:62.400957\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 24\n",
    "#read text from the \"input.txt\" file\n",
    "data_generator = DataGenerator('text.txt', sequence_length=sequence_length)\n",
    "gru =  GRU(hidden_size=100, vocab_size=data_generator.vocab_size,sequence_length=sequence_length,learning_rate=0.005)\n",
    "\n",
    "\n",
    "gru.train(data_generator,iterations=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car your shay maranke nmtur saman-se\\nThyu?\\n\\nOROLANUS:\\nYub nar meest sordis. de nbur this bler mse aby upars har um mes pspowe warth ua nmene,\\nAfeccushs'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.predict(data_generator, \"c\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>itsbeen</td>\n",
       "      <td>itsbeenachenosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vanilla LSTM</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-foursen frow't wis calestnow:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vanilla GRU</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-fourth. pure borncatl bu gray...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                  Input  \\\n",
       "0           RNN                itsbeen   \n",
       "1  Vanilla LSTM  It's been eighty-four   \n",
       "2   Vanilla GRU  It's been eighty-four   \n",
       "\n",
       "                                              Output  \n",
       "0                                itsbeenachenosaurus  \n",
       "1  It's been eighty-foursen frow't wis calestnow:...  \n",
       "2  It's been eighty-fourth. pure borncatl bu gray...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate output for GRU\n",
    "gru_output = gru.predict(data_generator, \"It's been eighty-four\", 150)\n",
    "\n",
    "# Add GRU results to the dataframe\n",
    "new_row = pd.DataFrame({\n",
    "    'Model': ['Vanilla GRU'],\n",
    "    'Input': [\"It's been eighty-four\"],\n",
    "    'Output': [gru_output]\n",
    "})\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vanilla GRU completed the input \"It's been eighty-four\" with \"It's been eighty-fourth.\", which already provides an existing (althought meaningless) word directly in completion the last word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Trial (Training on Titanic Script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're using the script from the movie \"Titanic\" to train our model.\n",
    "This will allow us to generate text that mimics (hopefully) the style and content of the famous film. First, we use the same configuration as before.\n",
    "\n",
    "The script was fetched from https://github.com/pratyakshs/Movie-script-parser/blob/master/t.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use the same configuration from before, solely training on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I(dF)xoGM(Is3cbe#zcEWh7l1\n",
      "jfw8f:vL)H13C)c\n",
      "(G0N83L'El('Q:U-zR)QzJ83VLCLVk5/GQ;fS/n.GC;o\"aOG-el)CX.\n",
      "gRuu.4dCZarI0BEik9KrzH!Gk(J\"B-oy92x?pzXJTZpY;G,L;B2cw//6mZED 8ZefQ-/t\n",
      "5D7.vdS#R'PR5M7Ud9s!7aCQ'J6\"ock'\n",
      "\n",
      "\n",
      "iter :0, loss:104.251186\n",
      "Tatmahvlh s onD 2 gs     .o oEoTUssiem  C  wv uesegav  oA feW  k sk wdut.,ma tehtSdbH .d   nlLs\n",
      "iaeg  v oEi ys d  l  s no   H La d i. eicLt  ir hua tg\n",
      "Ea amo Y  a  ,naLrhue.iphyo\n",
      " aaaSsiy b\n",
      "(Tma ib uh\n",
      "\n",
      "\n",
      "iter :1000, loss:85.105238\n",
      "nrog e\n",
      "oe srpsPnnkgot sCO loin  ye  f nYcTFed l,te e OidelpUB sya    ofsli \n",
      "..hag dcle   eletmvfel iisnn \n",
      "wcseegahKl wmiitnrotrRoldarEAiRsaoes!sisTeh\n",
      "  asognkoe.lt,a.et sil\n",
      "ihismil.afns lost \" oDnilht\n",
      "\n",
      "\n",
      "iter :2000, loss:76.541247\n",
      ". sh   \n",
      "oh    a   T     ,    cY  C( u RlBit t ofyhe mancg scgoI  u ind nswiI  ,)o\n",
      " gvn Gv  edhi N  th ooaEoeaocwheorg  a rchdeacee KufteiounehT h5skoe \n",
      "ea dllasd  cg J iben deka o o\n",
      " s eveew nhC e.hed\n",
      "\n",
      "\n",
      "iter :3000, loss:69.687841\n",
      "k s iRhyinradmlvthuhyendansthrw\n",
      "nrelmggtnpadut taregte ag nteO Heode hee oh itompcdCr1\n",
      "istJi deafrnc g oooRigMeJyapty aogas ii ans.Ohhenafsfint\n",
      "\n",
      "cd ineclehag\n",
      "\n",
      "e     d                          k       \n",
      "\n",
      "\n",
      "iter :4000, loss:61.561720\n",
      "oge duk t w.dhecun teisn,  ,  h skesfn- re \n",
      "hidelohprgmwitethtolaliu ihe gfdllosaye.u\n",
      "he  thiy ashe  enes\n",
      "\n",
      "g eatutrc aste .eBd tcas.\n",
      "n eaodoubultbanl  hichawhestep\n",
      "h. t  u gi!y t by  te ais ayon te fn\n",
      "\n",
      "\n",
      "iter :5000, loss:57.554878\n",
      "fdulnd otos rer.oveland  oderO.ebtCySz ane as riurs ouukherey eird athe  d\n",
      "ijt rbeH'tecuah nsih tmutodstis facel Dot sasgMed\n",
      "te sedhep.\n",
      "\n",
      "                                         R OEAADNse O Ai  !Ay\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iter :6000, loss:54.902987\n",
      " O E C1NMESEHIEbE:SAEBGHI fhoatm cM, pTAas gir thit. nasing imejnesand mt ziRipg rim murbucs nwo ate giny?dn nheslag! zobind  otes ndabaf gdrmand anhet, isy t Woes pnokiy, in hhhe admym.\n",
      "ilCjom ,an th\n",
      "\n",
      "\n",
      "iter :7000, loss:53.498494\n",
      "I P CA:DTLHWNleEangang bigty ard lle'tey. \n",
      "6Be the dearel . o Bthore. Bfin dir fag sther \n",
      "anhigfl ?omupisrapk .dewpa?nt  wrars ly chis, J' he songes an''s dhan'ns bisems mond inmdrt  ue tiangchas shet\n",
      "\n",
      "\n",
      "iter :8000, loss:51.101586\n",
      "t caks iud lup angl\n",
      "\n",
      "a  e fawe \n",
      "fHacadop. fong vawcs th dinisf the!dertre tee theomid, cken ify Iatdwmintoukds wat sery \n",
      "heaict  t'n but ayeg\n",
      "W. Ha sr toosocAMIilk sar 'u\"radsthes yoog thets!\n",
      "\n",
      "het rfo\n",
      "\n",
      "\n",
      "iter :9000, loss:49.520646\n",
      " S OGEL. INe the thic'nd ind bous the afeg Rild antllendr.\n",
      "\n",
      "Rotelsyes wo bre-ken see cou. E? Susottltre ondrbuep . ROk SiR, UTheste ches.\n",
      "\n",
      "\n",
      "Clok Danebet lous son7win ghilg\n",
      "\n",
      "s ou cos heand,  ha yet\n",
      "\n",
      "IN\n",
      "\n",
      "\n",
      "iter :10000, loss:49.250501\n",
      "lo fist, the o22uarhes raw. bheiling tn. bokrenl cackret hamed bhiye eit, adithang,.\n",
      "\n",
      "Aot aoveingery ang Dost of on eanilerbang thy wherof idering omerarino the crho\n",
      "lu s irkreIPsen hes aan rinde\n",
      "\n",
      "dhe\n",
      "\n",
      "\n",
      "iter :11000, loss:50.758380\n",
      "   C S cick and ti tigte stowche shepl awntings f arsorlang..\n",
      "\n",
      ", mee thes suck futy afd uf hearrden daseshen..\n",
      "\n",
      "houcg unt thas the traGtp.\n",
      "\n",
      "Th Ie kenrele. HOSLY RBl?Ybang shivybl ou heis fetlicr.\n",
      " he \n",
      "\n",
      "\n",
      "iter :12000, loss:50.328638\n",
      "l ond sifiingey. Ie hirigh. (o tl . ay deck, Dltise sars thier. warexoft\n",
      " . h 1AT VPOCUR EIs ha slo tht\n",
      " oo ino.\n",
      "\n",
      "                    CTU\n",
      "VN:E :BC:\n",
      "\n",
      "TSNis IIEGE . N7 IOM cAK.-\n",
      "\n",
      "                     L \n",
      "\n",
      "\n",
      "iter :13000, loss:50.749844\n",
      "s alyin ous taided uc's. co loorejrthe thetn.\n",
      "\n",
      "Rucor.\n",
      "f  rale she cing)\n",
      "K AG Ho DagAGIt s ingar fiom haoms her stkermame bebllapnOpldapgy hoa pithergang of faud, aldetcly sang n oupl\n",
      "at wadigihinas ar\n",
      "\n",
      "\n",
      "iter :14000, loss:49.507053\n",
      "GRN\n",
      "\n",
      "1TYLXI g JON Th ay aor The\n",
      " heck\n",
      "as weelg prpth, apykels ande sretre.\n",
      "\n",
      ", t rack a ther dolelingthvy tomesd a\n",
      "\n",
      "atercysaydy ar thitht. CId Bokz y 5acine potit altechllcot\n",
      "dastenge. anwund ind of th\n",
      "\n",
      "\n",
      "iter :15000, loss:48.447863\n"
     ]
    }
   ],
   "source": [
    "data_generator = DataGenerator('titanic.txt', sequence_length=sequence_length)\n",
    "lstm = LSTM(hidden_size=200, vocab_size=data_generator.vocab_size,sequence_length=sequence_length,learning_rate=1e-3)\n",
    "lstm.train(data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's been eighty-fourcw.\\n\\n                           S ORMES\\n\\nON ELD 3 CO'\\nN I NADGHU WA AROS EhC oW lesiet.\\n\\n                         a(Tecild tite wteryeg? ROSMINF RABTE\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.predict(data_generator, \"It's been eighty-four\", 150)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>itsbeen</td>\n",
       "      <td>itsbeenachenosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vanilla LSTM</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-foursen frow't wis calestnow:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vanilla GRU</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-fourth. pure borncatl bu gray...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned LSTM</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-four thod es Mo dofe tot.\\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model                  Input  \\\n",
       "0           RNN                itsbeen   \n",
       "1  Vanilla LSTM  It's been eighty-four   \n",
       "2   Vanilla GRU  It's been eighty-four   \n",
       "3    Tuned LSTM  It's been eighty-four   \n",
       "\n",
       "                                              Output  \n",
       "0                                itsbeenachenosaurus  \n",
       "1  It's been eighty-foursen frow't wis calestnow:...  \n",
       "2  It's been eighty-fourth. pure borncatl bu gray...  \n",
       "3  It's been eighty-four thod es Mo dofe tot.\\n\\n...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate output for LSTM\n",
    "lstm_output = lstm.predict(data_generator, \"It's been eighty-four\", 150)\n",
    "\n",
    "# Add LSTM results to the dataframe\n",
    "new_row = pd.DataFrame({\n",
    "    'Model': ['Tuned LSTM'],\n",
    "    'Input': [\"It's been eighty-four\"],\n",
    "    'Output': [lstm_output]\n",
    "})\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Tuned LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs we're seeing are initially \"gibberish\" as the model attempts to generate text based on what it has learned from the training data. As the LSTM model trains, it periodically generates sample text to show its progress, starting with random nonsense and gradually becoming more coherent. In early stages, the output looks like complete gibberish with random characters strung together. As training progresses, we might see more word-like structures, often nonsensical or misspelled. Towards the end of training, we start to see more recognizable words and some grammatical structures, though the overall text might still not make much sense. The predict function generates new text based on a given starting sequence, with the model attempting to continue the phrase \"It's been 84\" based on patterns learned from the Titanic script (inspired by the meme \"It's been 84 years...\"). The gibberish-like output occurs because the model is learning to predict character by character, requiring substantial training to produce coherent text. Even then, it won't produce perfect sentences, as it's merely mimicking the statistical patterns of characters from the training data. This type of output is normal and expected for character-level language models, especially with relatively small datasets and limited training time. With more data, longer training, and some tweaks to the model architecture, the output can become more coherent and human-like. That's why we oughta implement a more sophisticated method, with hyperparameters to tune, to generate more coherent text (a word based approach, on the other hand, should perform significantly better, but that's not the scope of this activity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Hyperparameter Tuning with Optuna\n",
    "\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    hidden_size = trial.suggest_int('hidden_size', 50, 300)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    sequence_length = trial.suggest_int('sequence_length', 10, 100)\n",
    "    \n",
    "    # Create data generator and model\n",
    "    data_generator = DataGenerator('titanic.txt', sequence_length=sequence_length)\n",
    "    lstm = LSTM(hidden_size=hidden_size, vocab_size=data_generator.vocab_size,\n",
    "                sequence_length=sequence_length, learning_rate=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    losses = []\n",
    "    for _ in tqdm(range(1000), desc=\"Training\"):  # Adjust number of iterations as needed\n",
    "        loss = lstm.train(data_generator)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_loss = np.mean(losses[-100:])  # Use average of last 100 losses\n",
    "    return test_loss\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Adjust number of trials as needed\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "data_generator = DataGenerator('titanic.txt', sequence_length=best_params['sequence_length'])\n",
    "lstm = LSTM(hidden_size=best_params['hidden_size'], vocab_size=data_generator.vocab_size,\n",
    "            sequence_length=best_params['sequence_length'], learning_rate=best_params['learning_rate'])\n",
    "\n",
    "# Train the model with progress bar\n",
    "lstm.train(data_generator)\n",
    "\n",
    "# Generate text\n",
    "generated_text = lstm.predict(data_generator, \"It's been 84\", 150)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)\n",
    " \"\"\"\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're utilizing GPU acceleration to train our LSTM model faster. By leveraging CUDA-enabled GPUs through PyTorch, we can perform parallel computations on large matrices, significantly speeding up both forward passes and backpropagation. This approach allows us to train deeper networks and process larger datasets in less time compared to CPU-only implementations.\n",
    "\n",
    "Ps.: I've tried doing this without using a GPU (with the previous configurations - shown in the commented code above), but after 12 hours of training, only 16 thousand iterations (aprox.) had been completed. That's why I decided to move on to this approach, which took roughly 12 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=vocab_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def train_model(self, data_generator, num_iterations=1000):\n",
    "        self.train()  # Ensure the model is in training mode\n",
    "        hidden = None\n",
    "        losses = []\n",
    "        pbar = tqdm(total=num_iterations, desc=\"Training\", ncols=100)\n",
    "        for i in range(num_iterations):\n",
    "            try:\n",
    "                inputs, targets = data_generator.next_batch()\n",
    "                inputs = inputs.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs, hidden = self(inputs, hidden)\n",
    "                loss = self.criterion(outputs.squeeze(0), targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)  # Gradient clipping\n",
    "                self.optimizer.step()\n",
    "\n",
    "                hidden = tuple(h.detach() for h in hidden)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "                if (i + 1) % 10000 == 0:\n",
    "                    print(f\"\\nIteration {i+1}/{num_iterations}\")\n",
    "                    sample = self.predict(data_generator, \"It's been 84\", 150)\n",
    "                    print(sample)\n",
    "                    print(f\"Loss: {loss.item():.4f}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                print(\"Skipping this iteration and continuing...\")\n",
    "                continue\n",
    "\n",
    "        pbar.close()\n",
    "        return losses\n",
    "\n",
    "    def predict(self, data_generator, start, n):\n",
    "        was_training = self.training\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, self.vocab_size, device=self.device)\n",
    "            hidden = None\n",
    "            idxes = [data_generator.char_to_idx[ch] for ch in start]\n",
    "            \n",
    "            for idx in idxes:\n",
    "                x[0, 0, idx] = 1\n",
    "                _, hidden = self(x, hidden)\n",
    "\n",
    "            for _ in range(n):\n",
    "                output, hidden = self(x, hidden)\n",
    "                prob = output.squeeze().div(0.8).exp()\n",
    "                idx = torch.multinomial(prob, 1).item()\n",
    "                \n",
    "                x = torch.zeros(1, 1, self.vocab_size, device=self.device)\n",
    "                x[0, 0, idx] = 1\n",
    "                idxes.append(idx)\n",
    "\n",
    "        if was_training:\n",
    "            self.train()\n",
    "        return ''.join(data_generator.idx_to_char[idx] for idx in idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, path: str, sequence_length: int):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "\n",
    "        chars = sorted(list(set(self.data)))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(chars)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.pointer = 0\n",
    "\n",
    "        # Use GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.sequence_length\n",
    "\n",
    "        inputs = [self.char_to_idx[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_idx[ch] for ch in self.data[input_start + 1:input_end + 1]]\n",
    "\n",
    "        # Convert to one-hot encoding\n",
    "        inputs_one_hot = torch.zeros(self.sequence_length, self.vocab_size, device=self.device)\n",
    "        inputs_one_hot[torch.arange(self.sequence_length), torch.tensor(inputs)] = 1\n",
    "\n",
    "        # Convert targets to tensor\n",
    "        targets = torch.tensor(targets, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # Update pointer\n",
    "        self.pointer += self.sequence_length\n",
    "        if self.pointer + self.sequence_length + 1 >= self.data_size:\n",
    "            self.pointer = 0\n",
    "\n",
    "        return inputs_one_hot, targets\n",
    "\n",
    "    def random_batch(self):\n",
    "        return self.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gvasc\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-09-29 19:47:24,411] A new study created in memory with name: no-name-81e14a3e-7700-4fe7-a81e-cd8ab97d75c2\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:06<00:00, 150.70it/s, loss=1.4635]\n",
      "[I 2024-09-29 19:47:32,142] Trial 0 finished with value: 1.665282855629921 and parameters: {'hidden_size': 216, 'learning_rate': 0.0024197790165067894, 'sequence_length': 74}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:04<00:00, 209.14it/s, loss=0.3921]\n",
      "[I 2024-09-29 19:47:36,931] Trial 1 finished with value: 2.1543486832827328 and parameters: {'hidden_size': 168, 'learning_rate': 0.005792084968666204, 'sequence_length': 16}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:05<00:00, 191.77it/s, loss=3.1265]\n",
      "[I 2024-09-29 19:47:42,153] Trial 2 finished with value: 3.0234732103347777 and parameters: {'hidden_size': 132, 'learning_rate': 0.00034242107941968625, 'sequence_length': 32}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:03<00:00, 259.18it/s, loss=2.4292]\n",
      "[I 2024-09-29 19:47:46,019] Trial 3 finished with value: 1.796503688627854 and parameters: {'hidden_size': 68, 'learning_rate': 0.008376495425378442, 'sequence_length': 18}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:06<00:00, 156.84it/s, loss=0.8277]\n",
      "[I 2024-09-29 19:47:52,402] Trial 4 finished with value: 2.43083332311362 and parameters: {'hidden_size': 64, 'learning_rate': 0.0030560391088928743, 'sequence_length': 15}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:06<00:00, 164.47it/s, loss=1.4497]\n",
      "[I 2024-09-29 19:47:58,492] Trial 5 finished with value: 1.8858283686637878 and parameters: {'hidden_size': 276, 'learning_rate': 0.001054702928800925, 'sequence_length': 73}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:03<00:00, 260.33it/s, loss=2.7006]\n",
      "[I 2024-09-29 19:48:02,339] Trial 6 finished with value: 2.2090025629661976 and parameters: {'hidden_size': 164, 'learning_rate': 0.0014241360150341927, 'sequence_length': 20}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:05<00:00, 190.96it/s, loss=2.0834]\n",
      "[I 2024-09-29 19:48:07,583] Trial 7 finished with value: 1.9808112694323063 and parameters: {'hidden_size': 171, 'learning_rate': 0.005523330274103106, 'sequence_length': 57}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:03<00:00, 275.70it/s, loss=3.1678]\n",
      "[I 2024-09-29 19:48:11,216] Trial 8 finished with value: 2.8156745487451555 and parameters: {'hidden_size': 168, 'learning_rate': 0.0004483973272956414, 'sequence_length': 20}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:05<00:00, 176.49it/s, loss=2.5504]\n",
      "[I 2024-09-29 19:48:16,889] Trial 9 finished with value: 2.1359428812377157 and parameters: {'hidden_size': 197, 'learning_rate': 0.0028084771994388523, 'sequence_length': 21}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:07<00:00, 141.77it/s, loss=2.8126]\n",
      "[I 2024-09-29 19:48:23,960] Trial 10 finished with value: 2.858795325756073 and parameters: {'hidden_size': 261, 'learning_rate': 2.5828810630851375e-05, 'sequence_length': 99}. Best is trial 0 with value: 1.665282855629921.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:03<00:00, 272.33it/s, loss=2.3272]\n",
      "[I 2024-09-29 19:48:27,647] Trial 11 finished with value: 1.6254360258579255 and parameters: {'hidden_size': 70, 'learning_rate': 0.009930194599319101, 'sequence_length': 77}. Best is trial 11 with value: 1.6254360258579255.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:06<00:00, 151.12it/s, loss=3.0921]\n",
      "[I 2024-09-29 19:48:34,281] Trial 12 finished with value: 2.877065898180008 and parameters: {'hidden_size': 234, 'learning_rate': 2.1410872354914667e-05, 'sequence_length': 80}. Best is trial 11 with value: 1.6254360258579255.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:02<00:00, 336.81it/s, loss=1.9886]\n",
      "[I 2024-09-29 19:48:37,264] Trial 13 finished with value: 2.8522587251663207 and parameters: {'hidden_size': 118, 'learning_rate': 0.00010406218837660199, 'sequence_length': 75}. Best is trial 11 with value: 1.6254360258579255.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:07<00:00, 136.52it/s, loss=2.1684]\n",
      "[I 2024-09-29 19:48:44,604] Trial 14 finished with value: 2.3263913333415984 and parameters: {'hidden_size': 221, 'learning_rate': 0.0009622272194127331, 'sequence_length': 56}. Best is trial 11 with value: 1.6254360258579255.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:03<00:00, 315.96it/s, loss=2.3099]\n",
      "[I 2024-09-29 19:48:47,783] Trial 15 finished with value: 2.684210571050644 and parameters: {'hidden_size': 104, 'learning_rate': 0.00010387580683692743, 'sequence_length': 93}. Best is trial 11 with value: 1.6254360258579255.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:04<00:00, 202.86it/s, loss=1.5777]\n",
      "[I 2024-09-29 19:48:52,727] Trial 16 finished with value: 1.8978935426473618 and parameters: {'hidden_size': 297, 'learning_rate': 0.00254267172248328, 'sequence_length': 45}. Best is trial 11 with value: 1.6254360258579255.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:06<00:00, 148.58it/s, loss=1.3407]\n",
      "[I 2024-09-29 19:48:59,473] Trial 17 finished with value: 1.5950941354036332 and parameters: {'hidden_size': 212, 'learning_rate': 0.009416018552945877, 'sequence_length': 84}. Best is trial 17 with value: 1.5950941354036332.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:02<00:00, 343.12it/s, loss=1.9653]\n",
      "[I 2024-09-29 19:49:02,403] Trial 18 finished with value: 1.5411496326327323 and parameters: {'hidden_size': 87, 'learning_rate': 0.008832015467223835, 'sequence_length': 90}. Best is trial 18 with value: 1.5411496326327323.\n",
      "Training: 100%|███████████████████████████████████| 1000/1000 [00:03<00:00, 315.78it/s, loss=1.7662]\n",
      "[I 2024-09-29 19:49:05,583] Trial 19 finished with value: 2.9127954268455505 and parameters: {'hidden_size': 100, 'learning_rate': 0.00012589972990398717, 'sequence_length': 85}. Best is trial 18 with value: 1.5411496326327323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_size': 87, 'learning_rate': 0.008832015467223835, 'sequence_length': 90}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|███▏                            | 10000/100000 [01:05<10:48, 138.78it/s, loss=1.7516]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 10000/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|███▎                             | 10026/100000 [01:06<26:47, 55.97it/s, loss=1.4568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been 84s\n",
      "pabins like a like Eil shoup of \n",
      "\n",
      "They robloom and trying the enorfis I trees some\n",
      "of the brocking. A TION OFFICER who wall, descer moster, of a cam\n",
      "Loss: 1.7516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██████▍                         | 20000/100000 [02:04<08:25, 158.35it/s, loss=0.5999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 20000/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██████▌                          | 20028/100000 [02:05<14:33, 91.56it/s, loss=1.4403]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been 84s hockete and alfice.\n",
      "\n",
      "                                               CUT TO:\n",
      "\n",
      "16 EXT. BOAT D / POV EXT... a KELDYSHOT of a stiad. The subed the littl\n",
      "Loss: 0.5999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|█████████▌                      | 30000/100000 [03:01<07:31, 154.94it/s, loss=1.9018]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 30000/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|█████████▉                       | 30024/100000 [03:02<16:22, 71.21it/s, loss=0.8618]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been 84s face.\n",
      "\n",
      "One best the most up and cover was about the ship's hanter she starts the deed.\n",
      "\n",
      "                          LOVETT\n",
      "\n",
      "The stading moons and 1912\n",
      "Loss: 1.9018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████████████▊                   | 40041/100000 [04:03<03:04, 324.94it/s, loss=0.7988]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 40000/100000\n",
      "It's been 84s eyes.\n",
      "\n",
      "                                                    LOVETT\n",
      "\n",
      "                                (to stangiting up forward fiuds. Swettlers of han\n",
      "Loss: 1.5620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████████████████                | 50000/100000 [04:56<05:21, 155.49it/s, loss=2.0238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 50000/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████████████████▌                | 50025/100000 [04:57<09:27, 88.14it/s, loss=1.2517]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been 84s a seaters. There is against a tomert of the everywhere's the wall.\n",
      "\n",
      "Jack was a Fabrizio piker.\n",
      "\n",
      "                                                    \n",
      "Loss: 2.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|███████████████████▏            | 60000/100000 [05:54<04:28, 149.21it/s, loss=1.4332]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 60000/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|███████████████████▊             | 60027/100000 [05:54<09:02, 73.73it/s, loss=0.8018]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been 84stobbite a greattans.\n",
      "\n",
      "Jack she sees the pointly, where or in the shipred, a\n",
      "comes at her and take at a pourt it like there, and Rose, his wantelized \n",
      "Loss: 1.4332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████████████████████▍         | 70040/100000 [06:51<01:32, 323.60it/s, loss=0.8900]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 70000/100000\n",
      "It's been 84 get don't looking someholation, with the reflection. Ruth do a gone put steerage now. Tomm'd time.\n",
      "\n",
      "Fabrizio is churred looking top water. He secress\n",
      "Loss: 0.5009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|█████████████████████████▌      | 80000/100000 [07:49<02:10, 152.98it/s, loss=0.7190]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 80000/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|██████████████████████████▍      | 80024/100000 [07:50<03:54, 85.01it/s, loss=0.8558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been 84 MORITTADED, to withourhelding me?\n",
      "\n",
      "                                                                                                        CUT TO:\n",
      "\n",
      "6\n",
      "Loss: 0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████████████████████████▊   | 90000/100000 [08:56<01:05, 152.46it/s, loss=1.6008]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 90000/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████▋   | 90026/100000 [08:56<02:12, 75.28it/s, loss=1.6382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been 84 to crew and hears up on the not ready but I see there strengtith, turming a straws the afficeer.\n",
      "\n",
      "                       JACK\n",
      "\n",
      "                     (\n",
      "Loss: 1.6008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 100000/100000 [09:57<00:00, 155.16it/s, loss=1.6027]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 100000/100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 100000/100000 [09:57<00:00, 167.38it/s, loss=1.6027]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been 84 wonder holy dresse asleeps pricker, which then. Instable of the stairs and you couldn't Jacu?\n",
      "\n",
      "                                                      \n",
      "Loss: 1.6027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "It's been 84 town that stome and beyond\n",
      "she saim against Cal, excitera picts and expassing the necklace behind, the right whites she be engress on. You got, and s\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 50, 300)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    sequence_length = trial.suggest_int('sequence_length', 10, 100)\n",
    "    \n",
    "    data_generator = DataGenerator('titanic.txt', sequence_length=sequence_length)\n",
    "    lstm = LSTM(hidden_size=hidden_size, vocab_size=data_generator.vocab_size,\n",
    "                sequence_length=sequence_length, learning_rate=learning_rate)\n",
    "    \n",
    "    losses = lstm.train_model(data_generator, num_iterations=1000)\n",
    "    \n",
    "    test_loss = np.mean(losses[-100:])\n",
    "    return test_loss\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "data_generator = DataGenerator('titanic.txt', sequence_length=best_params['sequence_length'])\n",
    "lstm = LSTM(hidden_size=best_params['hidden_size'], vocab_size=data_generator.vocab_size,\n",
    "            sequence_length=best_params['sequence_length'], learning_rate=best_params['learning_rate'])\n",
    "\n",
    "# Train the model\n",
    "lstm.train_model(data_generator, num_iterations=100000)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    lstm.eval()  # Set the model to evaluation mode\n",
    "    generated_text = lstm.predict(data_generator, \"It's been 84\", 150)\n",
    "    print(\"Generated text:\")\n",
    "    print(generated_text)\n",
    "    lstm.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>itsbeen</td>\n",
       "      <td>itsbeenachenosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vanilla LSTM</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-foursen frow't wis calestnow:\\nAnd this sext manmanmalirgo: in the lond; you hoss and suicl: wersing aikn!\\nI pow, hereall here trye must.\\n\\nDUKE VINCENTIO:\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vanilla GRU</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-fourth. pure borncatl bu gray ugarst ke.\\n Mard pore feasbe ws hances af is disercims s fold Ifto tobrus?\\nby! us dmom sesave ftrespres sheve fralistn-y-ucn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned LSTM</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-four thod es Mo dofe tot.\\n\\nNas. RWansa save farosemes te steicho doyckes shing tha ta eures movee tleas lost fooun\\n. at Japile Vbackdoogs d aSSe soth skil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hyper-Tuned LSTM</td>\n",
       "      <td>It's been eighty-four</td>\n",
       "      <td>It's been eighty-foursdaling to the drawing in the stairs.\\n\\n                                                                    CUT TO:\\n\\n745 EXT. TERN stuff and you're she</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model                  Input                                                                                                                                                                            Output\n",
       "0               RNN                itsbeen                                                                                                                                                               itsbeenachenosaurus\n",
       "1      Vanilla LSTM  It's been eighty-four  It's been eighty-foursen frow't wis calestnow:\\nAnd this sext manmanmalirgo: in the lond; you hoss and suicl: wersing aikn!\\nI pow, hereall here trye must.\\n\\nDUKE VINCENTIO:\\n\n",
       "2       Vanilla GRU  It's been eighty-four     It's been eighty-fourth. pure borncatl bu gray ugarst ke.\\n Mard pore feasbe ws hances af is disercims s fold Ifto tobrus?\\nby! us dmom sesave ftrespres sheve fralistn-y-ucn\n",
       "3        Tuned LSTM  It's been eighty-four    It's been eighty-four thod es Mo dofe tot.\\n\\nNas. RWansa save farosemes te steicho doyckes shing tha ta eures movee tleas lost fooun\\n. at Japile Vbackdoogs d aSSe soth skil\n",
       "4  Hyper-Tuned LSTM  It's been eighty-four   It's been eighty-foursdaling to the drawing in the stairs.\\n\\n                                                                    CUT TO:\\n\\n745 EXT. TERN stuff and you're she"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate output for LSTM\n",
    "lstm_output = lstm.predict(data_generator, \"It's been eighty-four\", 150)\n",
    "\n",
    "# Add LSTM results to the dataframe\n",
    "new_row = pd.DataFrame({\n",
    "    'Model': ['Hyper-Tuned LSTM'],\n",
    "    'Input': [\"It's been eighty-four\"],\n",
    "    'Output': [lstm_output]\n",
    "})\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# Display the dataframe with all columns visible\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "display(df)\n",
    "\n",
    "# Reset display options to default (optional)\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.expand_frame_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Breakdown\n",
    "\n",
    "First, let's describe each of the previous models created:\n",
    "\n",
    "1. RNN (Recurrent Neural Network):\n",
    "   - A basic recurrent model that processes sequences.\n",
    "   - Tends to have issues with long-term dependencies.\n",
    "   - Trained on the **dinos.txt** \"dataset\".\n",
    "\n",
    "2. Vanilla LSTM (Long Short-Term Memory):\n",
    "   - An improvement over RNN, designed to handle long-term dependencies better.\n",
    "   - Uses gates to control information flow.\n",
    "   - Trained on the **text.txt** \"dataset\".\n",
    "\n",
    "3. Vanilla GRU (Gated Recurrent Unit):\n",
    "   - Similar to LSTM but with a simpler architecture.\n",
    "   - Often performs comparably to LSTM with fewer parameters.\n",
    "   - Also trained on the **text.txt** \"dataset\".\n",
    "\n",
    "4. Tuned LSTM:\n",
    "   - Aims to improve performance over the vanilla LSTM (toward the new dataset).\n",
    "   - Now trained on the **titanic.txt** \"dataset\" (the script from the movie titanic).\n",
    "\n",
    "5. Hyper-Tuned LSTM:\n",
    "   - An LSTM model with hyperparameters optimized using Optuna.\n",
    "   - Represents our most advanced model, leveraging automated hyperparameter tuning.\n",
    "   - Also trained on the **titanic.txt** \"dataset\".\n",
    "\n",
    "Each subsequent model aims to improve upon the previous ones in terms of text generation quality and coherence. However, a significant limitation of character-based RNN models is their inability to capture higher-level semantic meaning. These models operate at the individual character level, which can lead to several issues:\n",
    "\n",
    "1. Limited context: Character-based models struggle to understand broader context, often resulting in nonsensical or grammatically incorrect sequences.\n",
    "2. Inefficiency: They require longer sequences to represent meaningful content, increasing computational demands.\n",
    "3. Lack of word-level understanding: These models don't inherently grasp word boundaries or meanings, making it challenging to generate coherent, meaningful text.\n",
    "4. Difficulty with rare characters: Uncommon characters or punctuation can disproportionately influence the model's predictions.\n",
    "5. Slower convergence: Character-level models typically require more training time to achieve comparable results to word-level models.\n",
    "\n",
    "That is why, in the results shown in the notebook, they manage to capture the structure of the text in which they were trained on, but rarely generate meaningful phrases.\n",
    "\n",
    "For the last model (Hyper-Tuned LSTM), at least, as it was given the \"best\" (debatable) hyper-parameters to solve the task at hand, it learned to generate meaninful words (even with a somewhat \"robust\" semantic meaning, forming phrases), and not only random letters (i.e *\"It's been eighty-foursdaling <u>to the drawing in the stairs</u>.\\n\\n CUT TO:\\n\\n745 EXT. TERN stuff and you're she\"*), when fed the input \"*It's been eighty-four*\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ps.: On iteration 40000/100000, you can see  that we got \"It's been 84s eyes.\" as a results. *Close enough... (sight)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
